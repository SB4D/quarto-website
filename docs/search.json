[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I’m a mathematician with roughly a decade of university level research and teaching under my belt. I also like computers and I’ve decided that this is what I want to do now.\n\nMath\nMy research area is the topology of 4-dimensional manifolds within the broader field of geometry and topology. If you’re also interested in these things, you may have seen my name on the cover of a book. You can find more about my research in the math section.\n\n\nComputers\nSince I’ve left academia, I’ve been on a deep dive into data science, machine learning, and related areas of software development. I’ve been working on a few projects and I will describe them in more detail in the projects section and blog.\n\n\nMusic\nMusic has always been my hobby but lately it has also served as an inspiration for coding projects.  I’m currently exploring various concepts of deep learning in the context of computer aided analysis of music and audio signals. I’ve also been getting more into digital signal processing (DSP) and music information retrieval (MIR) and this will be reflected in various posts in my blog."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Code/Projects",
    "section": "",
    "text": "This page is a showcase for some of my software projects. Source code is available on my GitHub page. My main programming language is Python, but I’ve also worked with Lua and a bit of C++.\n\nsb4dfritz — Tools for AVM FRITZ! Home Automation Systems\n\nGitHub Repository: github.com/SB4D/sb4dfritz \n\nsb4dfritz provides specialized functionality to interact with home automation devices connected to AVM FRITZ!Box routers. This is mainly a learning project for object oriented programming in Python and network communication using HTTP and SOAP. Originally, the communication was handled by the fritzconnection library, but I have written my own library sb4dfritzlib to remove this dependency. As a next step, I’m planning to add a browser-based frontend.\n\n\nsb4dportfolio — Python and LaTeX for Job Applications\nI took quite some pleasure in designing my application material whilst honing my computer skills. I was looking for a solution that is visually appealing, especially on computer screens, and easy to use (for me). In particular, I wanted to separate design aspects and content as much as possible. This requirement made LaTeX and Python a natural choice. As a bit of eye candy, I had the idea for a subtle watermark image fading into the page, with the added personal touch of transitioning from mathematics into code.\n\nWatermark Design: To get the watermark exactly right, I wrote a Python library for image processing (based on PIL and NumPy) which offers blending and fading effects with fully customizable gradients.\nLaTeX Template Design: I then designed LaTeX templates for cover letters and CVs with my watermark embedded.\nAutomatic Document Generation: Having created the CV template, I wrote a Python script that automatically generates a LaTeX CV from data stored in a YAML file.\n\nNow all I have to do is write cover letters in plain text, keep my CV data up to date in the suitably structured YAML file, and run the scripts. No more fussing around with annoying table structures and formatting options in Word or LaTeX.\n\n\nHow did I do? — A MIDI Drum Practice Tool\nWhile I’m not a drummer myself, I picked up finger drumming for sketching songs. This brought me to Melodics, which I’ve briefly used. While I liked its concept, it was too expensive and felt too much like a toy. I wanted a real practice tool that would allow me to evaluate my finger drum performances and help me to improve my sense of rhythm and timing accuracy. As a quick fix, I developed a rudimentary implementation as an extension for REAPER written in Lua using the ReaScript API. The goal is to make this a self-contained web application using Python, FastAPI and a bit of JavaScript for the Web MIDI API.\n\n\nsbehrens4d.com — This Website\nLastly, there’s my website. I know, exactly what the world needed… Well, I felt like I needed one. In part, it’s meant to serve as an extension of my application material, so that potential employers can get a better idea of who I am and what I (can) do. The website is based on Quarto. I started with a template and added a few tweaks here and there. Two notable features:\n\nThe background image is another application of my image processing library mentioned above and references the watermark in my application material.\nSimilarly, the CV page is generated from the same YAML file used for my LaTeX CV."
  },
  {
    "objectID": "posts/making_of_homepage.html",
    "href": "posts/making_of_homepage.html",
    "title": "Made a website using Quarto",
    "section": "",
    "text": "Well, I made another website. You’re looking at it right now. While I’ve fumbled around with the good old HTML/CSS/JavaScript combo before, I’ll openly admit that I’m no expert in webdesign. This time around, I’m using Quarto. As you an see, I’ve also added a blog and I’ve registered my own domain name. If you’re interested, let me walk you through how this thing came to be."
  },
  {
    "objectID": "posts/making_of_homepage.html#making-it-happen",
    "href": "posts/making_of_homepage.html#making-it-happen",
    "title": "Made a website using Quarto",
    "section": "Making it Happen",
    "text": "Making it Happen\n\nCreating a Website with a Blog\nSetting up a website in Quarto is easy. If you’re also using VS Code, just install the Quarto software and the VS Code extension on your machine, and follow these instructions from the Quarto documentation.\nIn a nutshel, a Quarto homepage appears to you as its author as a folder structure mostly consisting of plain text files with the extention .qmd which is short for “Quarto markdown”. More on that later. In addition, there is an all-important file calle _quarto.yml which instructs the Quarto software how to render the .qmd files into HTML. Here’s what this looks like for this website (at the time of writing):\n\n\n_quarto.yml\n\nproject:\n  type: website\n  preview:\n    port: 2025\n\nwebsite:\n  title: \"Stefan Behrens | Mathematician | Deep Learner\"\n  navbar:\n    right:\n      - href: index.qmd\n        text: Home\n      - href: cv.qmd\n        text: CV\n      - href: math.qmd\n        text: Math/Research\n      - href: projects.qmd\n        text: Code/Projects\n      - href: blog.qmd\n        text: Blog\n  draft-mode: gone\n\nformat:\n  html:\n    theme:\n      - flatly\n    css: styles.css\n    toc: true\n    max-width: 800px\n    highlight-style: tango\n    embed-resources: false\n    include-in-header: _tools/sb4dlatex.html\n\nI’m not going to go into any detail, all I want to say is that this doesn’t look too complicated.\n\n\nAdding a blog\nAdding a blog is just as easy and is explained here. You just have to create a subdirectory for blog posts, say posts/, and .qmd file containing something like this:\n\n\nblog.qmd\n\n---\nlisting:\n    contents: posts\n    sort: \"date desc\"\n    type: default\n    categories: true\n    sort-ui: false\n    filter-ui: false\npage-layout: full\ntitle-block-banner: true\n---\n\nThat’s it. These instructiosn (written in YAML again) tell Quarto everything it needs to know to render a blog page with posts correspondin to files in the posts/ directory. These can be either .qmd files or Jupyter Notebooks. More on adding content later.\n\n\nStyling with CSS\nAs I mentioned, while Quarto pages are not written in HTML, they are ultimately rendered as HTML. As you can see, my _quarto.yml references a file styles.css. This used to style the rendered HTML pages, so that you can work your CSS magic on your homepage. In my case, this isn’t much:\n\n\nstylels.css\n\n/* css styles */\n\n/* Custom page background */\nbody {\n  background-image: url(\"images/bg_main.png\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-position: center;\n}\n\n/* CV stuff */\n.cv-entry { \n  display: block;\n}\n  .cv-entry summary {\n    color: black;\n    cursor: pointer;\n  }\n  .cv-entry .title{\n    font-size: large;\n    font-weight: bold;\n  }\n  .cv-entry .titleinfo{\n    font-size: large;\n  }\n  .cv-entry .subtitle{\n    font-style: italic;\n    margin-top: 0.1em;\n  }\n  .cv-entry .date{\n    float: right;\n  }\n  .cv-entry .details {\n    background-color: #f3f7ff;\n    border-radius: 10px;\n    margin: 0.5em 0 0 1em;\n  }\n    .cv-entry details p {\n      margin: 0 0 0 1em;\n      padding: 0;\n    }\n\nEssentially, I’m only adding a background image and a few custom styles for the CV page. But I’ll get to that.\n\n\nAdding Content\nAlright, enough about the structure. Let’s talk content.\n\nQuarto Markdown\nOne of the selling points for me was that I don’t have to write pages in HTML. I much prefer writing *this* over writing &lt;b&gt;that&lt;/b&gt;. Maybe it’s related to the German keyboard layout, but I’ve alwasy found HTML extra awkward to type. As for the Quarto markdown syntax, it’s bascially the same as the familiar markdown from GitHub and Jupyter Notebooks. It’s cerainly less flexible that HTML, but it gets the job done for basic texts. It’s also possible to use LaTeX, just as you would in Jupyter Notebooks.\n\n\nUsing HMTL in Quarto pages\nIf Quarto markdown is not flexible enough for you, the good news is that Quarto allows you to use HTML directly in the .qmd files. For the most part, you can just write it straight into the file. However, for more complicated constructs, you might have to wrap it into a raw HTML code block like this:\n```{=html}\n&lt;div class=\"cv-entry\"&gt;\n&lt;details&gt;\n    &lt;summary&gt;\n        &lt;span class=\"title\"&gt;Great Job&lt;/span&gt;\n        &lt;span class=\"date\"&gt;2024 — Present&lt;/span&gt;\n        &lt;p class=\"subtitle\"&gt;Prestigious Company — Somewhere Fancy&lt;/p&gt;\n    &lt;/summary&gt;\n&lt;div class=\"details\"&gt;\n&lt;ul&gt;\n&lt;li&gt;Awesome thing I did.&lt;/li&gt;\n&lt;li&gt;More greatness.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/details&gt;\n&lt;/div&gt;\n```\nNaively, I expected this to work without the triple backticks. But for some reason, Quarto didn’t want to render this properly. The content wrapped in &lt;div class=\"cv-details\"&gt;...&lt;/div&gt; was displayed verbatim.\nAfter a frustrating conversation with ChatGPT, I went the classical RTFM way and found this. Turns out you have to explicitly announce to quarto that raw HTML is coming.\n\n\n\nJupyter Notebooks and Blog Posts\nWhile choosing a blogging platform, my main requirement was that I can recycle my already existing Jupyter Notebooks as blog posts with as little hassle as possible. This is where Quarto really shines. As explained here, you just have to make sure to that the first cell in your .ipynb file is “raw” and contains a suitable YAML front matter such as:\n---\ntitle: \"Made a new website using Quarto\"\nauthor: \"Stefan Behrens\"\ndate: \"8/3/2025\"\ncategories:\n  - Webdesign\n  - Quarto\n  - CSS\n---\nWith this in place, just copy the .ipynb into your blog’s posts folder and you’re good to go. Occassionally, you might have to run the entire notebook and save it.\nAlternatively, you can write blog posts as Quarto markdown .qmd files. They also need a YAML front matter as above.\n\n\nUsing LaTeX in Quarto\nAs mentioned, Quarto markdown has native LaTeX support. By default, it uses MathJax but it’s possible to use other rendering engines such as KaTeX. Since I’ve been using LaTeX for many years to write mathematical texts, I’ve gotten used to a couple of custom commands. Luckily, there are ways to use custom commands in Quarto pages. I’ve written more about this in another post."
  },
  {
    "objectID": "posts/making_of_homepage.html#getting-it-online",
    "href": "posts/making_of_homepage.html#getting-it-online",
    "title": "Made a website using Quarto",
    "section": "Getting it Online",
    "text": "Getting it Online\nRight, now that the website has everything it needs, it’s time to get it online. There are several ways to do so, both free and paid. After doing some research, I opted for the free option offered by GitHub.\n\nHosting on GitHub Pages\nGitHub offers a service called Pages which can be used to host websites within a repository. At the time of writing, it is even available with the free plan, albeit with a caveat. It works roughly like this. Say you have a repository my-repo with a directory repo_site/ containing data for a website. You can then instruct GitHub Pages to make that folder available under https://username.github.io/my-repo. However, there are some limitations:\n\nAs far as I can tell, the websites have to be static. Maybe you can play some tricks to get dynamic content going, but I don’t know.\nImportant: If you’re on GitHub’s free plan, the repository must be public from here onward!\n\n\nMethod 1: Render Locally, Upload Everything\nAssuming that you have the Quarto engine running locally, the easiest way to the get your website published on GitHub Pages.\n\nCreate a GitHub account if you don’t have one, yet.\nGet everything ready locally:\n\nAdd this to your _quarto.yml:\n\nproject:\n  # whatever you already had in this section\n  output-dir: docs\n\nMake sure you have a .gitignore file containing the line /.quarto/\nRender the project. The pages is now rendered to the subdirectory docs/. Delete the previous output directory (the default is _site/).\nClean up your project folder. Remove everything you don’t want anyone to see.\n\nPublish the project folder as a GitHub repository, e.g. my-quarto-site.\n\nAgain, if you’re on the free plan, the repository needs to be public!\n\nActivate GitHub Pages:\n\nLog into your GitHub Account, navigate to the website repository.\nGo the setting tab and navigate to the “Pages” register\nMake the following settings:\n\nBuild and Deplyment / Source -&gt; Deploy from a branch\nBuild and Deplyment / Branch -&gt; Choose branch (most likely main)\nBuild and Deplyment / Branch -&gt; Select folder docs/\nBuild and Deplyment / Branch -&gt; Click “Save”\n\n\nCheck if your page is online:\n\nNavigate to githubusername.github.io/my-quarto-site. Your page should appear.\n\n\n\n\nMethod 2: Upload Sources, Render Using GitHub Actions\nThere’s another way to do things that doesn’t require you to render your Quarto site locally. You can have it rendered on GitHub’s servers instead. However, the setup is a little more involved. Since I’m fine rendering locally, I haven’t tried this, yet.\nAnyway, here’s what ChatGPT told me about the process. Again, I haven’t tried it! I’m posting it here partly to remind myself that I wanted to try his eventually.\n\nAlright — here’s a ready-to-use GitHub Actions workflow that will build and deploy your Quarto site automatically to &gt; GitHub Pages.\nYou just need to:\n\nPut this file in your repo at .github/workflows/publish.yml.\nPush it to GitHub.\nEnable Pages to serve from the gh-pages branch (instead of /docs).\n\n\n.github/workflows/publish.yml\nname: Publish Quarto site to GitHub Pages\n\non:\n  push:\n    branches: [main]  # Build on every push to main branch\n  workflow_dispatch:   # Allow manual trigger from Actions tab\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n\n    permissions:\n      contents: write   # Needed for pushing to gh-pages branch\n      pages: write\n      id-token: write\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          version: \"latest\"\n\n      - name: Set up Pandoc (optional if Quarto needs it)\n        uses: pandoc/actions/setup@v1\n\n      - name: Install LaTeX (optional, only if you render PDFs)\n        run: sudo apt-get install -y texlive-latex-base\n\n      - name: Render Quarto project\n        run: quarto render\n\n      - name: Deploy to GitHub Pages\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./_site   # This is Quarto's default output dir\n\nHow to set it up\n\nRemove output-dir: docs from _quarto.yml (so Quarto builds to _site).\nCommit this publish.yml workflow to .github/workflows/.\nIn GitHub → Settings → Pages:\n\nSource → Select Deploy from a branch.\nBranch → gh-pages (created by the workflow after first run).\n\nPush your changes to main.\nWait for the Actions workflow to finish — your site will appear at:\nhttps://&lt;your-username&gt;.github.io/&lt;your-repo&gt;/\n\n\nThat’s all for now. Once this is online, I’m planning to redirect this page to a domain which I’ve already registered. I’ll update this post."
  },
  {
    "objectID": "posts/making_of_homepage.html#to-be-continued",
    "href": "posts/making_of_homepage.html#to-be-continued",
    "title": "Made a website using Quarto",
    "section": "To Be Continued…",
    "text": "To Be Continued…"
  },
  {
    "objectID": "posts/latex_in_jupyter_notebooks.html",
    "href": "posts/latex_in_jupyter_notebooks.html",
    "title": "Custom LaTeX Commands in Jupyter Notebooks",
    "section": "",
    "text": "As a mathematician I’ve been using LaTeX for a very long time. Over the years I’ve gotten used to a number of custom macros. Now that I’m using Jupyter Notebooks more and more, I’ve been looking for a convenient way to use my commands in this setting as well. Most importantly, I’ve internalized the commands below to an extent that I will type them even if a try not to.\nBy the way, if you’re wondering why I’m using \\providecommand instead of \\newcommand or \\renewcommand, I’ve found the former to be less error prone since it doesn’t try to overwrite or override already existing commands. And it gets the job done just as well for me."
  },
  {
    "objectID": "posts/latex_in_jupyter_notebooks.html#things-that-work-from-the-get-go",
    "href": "posts/latex_in_jupyter_notebooks.html#things-that-work-from-the-get-go",
    "title": "Custom LaTeX Commands in Jupyter Notebooks",
    "section": "Things that work from the get-go",
    "text": "Things that work from the get-go\nBefore I continue, I should say that Visual Studio Code is my editor of choice. This matters, because it uses KaTeX as a rendering engine for LaTeX code in Jupyter Notebooks. KaTeX is not a full LaTeX distribution and only supports a small selection of commands. The full list is available here. A few of “my” commands work from the get-go:\n\n\\R gives \\(\\R\\)\n\\Z gives \\(\\Z\\)\n\\N gives \\(\\N\\)\n\\set{...} serves as \\{...\\} and gives \\(\\set{...}\\)\n\\Set{...} serves as \\left\\{...\\right\\} and curly brackets with auto-adapted size\n\nHowever, \\Q and \\C do not give \\(\\mathbb Q\\) and \\(\\mathbb C\\)! While I don’t use \\Q that much, not having \\C is a problem."
  },
  {
    "objectID": "posts/latex_in_jupyter_notebooks.html#adding-custom-latex-commands-in-a-jupyter-notebook",
    "href": "posts/latex_in_jupyter_notebooks.html#adding-custom-latex-commands-in-a-jupyter-notebook",
    "title": "Custom LaTeX Commands in Jupyter Notebooks",
    "section": "Adding custom LaTeX commands in a Jupyter Notebook",
    "text": "Adding custom LaTeX commands in a Jupyter Notebook\nFortunately, it is possible to define custom LaTeX macros in a Jupter notebook. You just have to wrap the usual \\newcommand{}[]{} or providecommand{}[]{}in dollar signs as in the following example:\n$\\newcommand{\\C}{\\mathbb C}$\nNow I can write $\\C$ instead of $\\mathbb C$!\n$\\providecommand{\\mc}[1]{\\mathcal{#1}}$\nNow I can write $\\mc H$ instead of $\\mathcal{H}$.\n\\(\\providecommand{\\C}{\\mathbb C}\\) \\(\\providecommand{\\mc}[1]{\\mathcal{#1}}\\) Indeed \\C and \\mc H now give \\(\\C\\) and \\(\\mc H\\). Once declared, the macros are available in the entire notebook; they are not limited to the cell in which they are defined.\nIndeed, this is a new cell and \\C still gives \\(\\C\\) although this was defined in a different cell. I could now even use this command before it was defined, that is, in cells above the one in which it is defined. However, this would produce an error the next time I open the notebook.\n\\(\\providecommand{\\Q}{\\mathbb{Q}}\\) There’s another minor catch. When using \\newcommand as in $\\newcommand{\\Q}{\\mathbb{Q}}$, say, make sure to put it in the top of the cell before any text. If you put it somewhere else (e.g. between paragraphs), then everything will be fine the first time you execute the cell, but starting from the second execution you’ll get this error:\nParseError: KaTeX parse error: \\newcommand{\\Q} attempting to redefine \\Q; use \\renewcommand\nAs I’ve already mentioned, I generally prefer to use \\providecommand{} as it’s less error prone. This also holds in this case. $\\providecommand{\\Q}{\\mathbb{Q}}$ will simply define \\Q the first time around and do nothing any time after that, no matter where it was placed in the code.\nTo summarize, the best practice is as in LaTeX: * Put all your macros in the beginning of the document before the main text. * Try to use \\providecommand instead of \\newcommand or \\renewcommand whenever you can.\nFrom now on, the first markdown cell in my Jupyter Notebooks will start something like this:\n# Title (or no title...)\n$$\n% Numbers\n\\providecommand{\\N}{\\mathbb{N}} % natural numbers\n\\providecommand{\\Z}{\\mathbb{Z}} % integers\n\\providecommand{\\Q}{\\mathbb{Q}} % rational numbers\n\\providecommand{\\R}{\\mathbb{R}} % real numbers\n\\providecommand{\\C}{\\mathbb{C}} % complex numbers\n\n% Math font styles\n\\providecommand{\\mbb}[1]{\\mathbb{#1}}\n\\providecommand{\\mc}[1]{\\mathcal{#1}}\n\\providecommand{\\mf}[1]{\\mathfrak{#1}}\n\n% Arrows\n\\providecommand{\\xra}[1]{\\xrightarrow{#1}}\n\n% Useful things\n\\providecommand{\\inv}{^{-1}} % inverses\n\\providecommand{\\del}{\\partial}\n$$\n\nText goes here...\n\nLoading custom LaTeX macros from a file into a Jupyter Notebook?\nAs I’ve said, over the years I’ve accumulated a set selection of custom macros and I’ve put them into a custom package. It would be great if I could use it in my Jupyter Notebooks as well. Unfortunately, it’s not as simple as writing $\\include[my_package]$. Neither can I simply put my commands in a file my_commands.tex and write $\\input{my_commands.tex}$.\nI’ll look into this another time and if I find a solution, I’ll update the article."
  },
  {
    "objectID": "math.html",
    "href": "math.html",
    "title": "Math/Research",
    "section": "",
    "text": "Welcome to the math section! This page is mainly written for other mathematicians. If you’re looking for layman’s accounts on mathematics, check if my blog has something for you."
  },
  {
    "objectID": "math.html#research-interests",
    "href": "math.html#research-interests",
    "title": "Math/Research",
    "section": "Research Interests",
    "text": "Research Interests\nMy field of research is low dimensional topology with a focus on 3- and 4-dimensional manifolds and applications of gauge theoretic invariants. I’m particularly interested in the interplay of topology, geometry, analysis, and algebra in relation to intersection forms of smooth 4-manifolds.\nI also enjoy thinking about purely structural features of the aforementioned gauge theoretic invariants. I’ve been exploring this from the perspective of equivariant stable homotopy theory which I’ve found to be a fascinating (but also confusing) subject in its own right.\nIf you think of mathematics as a folder tree, here’s roughly where to locate me and my contributions:\nMathematics\n└── Pure Mathematics\n    └── Geometry & Topology\n        └── Topology of Manifolds\n            └── Low Dimensional Topology\n                └── 3- and 4-dimensional Manifolds\n                    ├── Topological 4-Manifolds\n                    │   └── Freedman's Work\n                    ├── Smooth 4-Manifolds\n                    │   ├── Diagrammatic Descriptions\n                    │   └── Donaldson's Theorem and its Generalizations\n                    └── Applications of Gauge Theory\n                        ├── Heegaard-Floer Theory\n                        │   └── Correction Terms and their Applications\n                        └── Seiberg-Witten Theory\n                            ├── Froyshov Invariants and their Generalizations\n                            └── Seiberg-Witten-Floer Homotopy Types\n\nOngoing Projects\n\n\n\n\n\n\nGeneralized Froyshov Invariants\n\n\n\n\n\nThis is mostly joint work with Tyrone Cutler. We’ve been developing a unified framework to study Froyshov-type invariants derived from the Seiberg-Witten-Floer homotopyes of rational homology 3-spheres and equivariant cohomology theories. We highlight the roles of Euler classes and orientations of \\(G\\)-representations where \\(G\\) is either the circle group \\(\\mathbb{T}=U(1)\\) or its normalizer \\(Pin(2)\\) in \\(SU(2)\\). Along the way, we give proofs of various folklore statements about the use of incomplete universes for the groups.\n\n\n\n\n\n\n\n\n\nSeiberg-Witten-Floer Homotopy Types for General 3-Manifolds\n\n\n\n\n\nThis is a long-term project joint with Thomas Kragh and Alice Hedenlund. Seiberg-Witten-Floer (SWF) homotopy types are supposed to refine monopole Floer homology. This is reasonably well understood for rational homology 3-spheres by the work of Manolescu and Lidman-Manolescu. Moreover, Khandhawit-Lin-Sasahira and Sasahira-Stoffregen have constructed SWF homotopy types for more general 3-manifolds, but the relation to monopole Floer homology is only conjectural. Another loose end is the functoriality with respect to 4-dimensional cobordisms. We set out to construct SWF homotopy types for general closed 3-manifolds which allow a clean framework to discuss cobordism maps and folding-unfolding phenomena related to local coefficients in monopole Floer homology. The key is a suitable notion of twisted parameterized equivariant homotopy types which we are developing."
  },
  {
    "objectID": "math.html#publications",
    "href": "math.html#publications",
    "title": "Math/Research",
    "section": "Publications",
    "text": "Publications\n\nBooks\n\n\n\n\n\n\nThe Disk Embedding Theorem (joint with B. Kalmar, M.-H. Kim, M. Powell and A. Ray)\n\n\n\n\n\n(Oxford University Press, 2021) [DOI] [Amazon]\nA detailed account on Freedman’s work on the 4-dimensional version of the Poincaré conjecture and its applications to the theory of topological 4-manifolds.\n\n\n\n\n\nArticles\n\n\n\n\n\n\nThe monopole h-invariants from a topological perspective (Preprint)\n\n\n\n\n\n[view on ar\\(\\chi\\)iv]\nAbstract: We study the monopole h-invariants of 3-manifolds from a topological perspective based on Lidman and Manolescu’s description of monopole Floer homology in terms of Seiberg-Witten-Floer homotopy types. We investigate the possible dependence on the choice of coefficients and give proofs of several properties of the h-invariants which are well known to experts, but hard to track down in the literature.\n\n\n\n\n\n\n\n\n\nHeegaard Floer correction terms, with a twist (joint with M. Golla)\n\n\n\n\n\n(Quantum Topol., Vol. 9 (2018), No. 1, 1-37) [DOI] [ar\\(\\chi\\)iv]\nAbstract: We use Heegaard Floer homology with twisted coefficients to define numerical invariants for arbitrary closed 3-manifolds equipped torsion spin structures, generalising the correction terms (or \\(d\\)-invariants) defined by Ozsváth and Szabó for integer homology 3-spheres and, more generally, for 3-manifolds with standard \\(\\mathrm{HF}^\\infty\\). Our twisted correction terms share many properties with their untwisted analogues. In particular, they provide restrictions on the topology of 4-manifolds bounding a given 3-manifold.\n\n\n\n\n\n\n\n\n\nClassification of boundary Lefschetz fibrations over the disc (joint with G. Cavalcanti and R. Klaasse)\n\n\n\n\n\n(“Geometry and Physics: A Festschrift in Honour of Nigel Hitchin, Volume II”, Oxford University Press 2018) [DOI] [ar\\(\\chi\\)iv]\nAbstract: We show that a four-manifold admits a boundary Lefschetz fibration over the disc if and only if it is diffeomorphic to \\(S^1 \\times S^3\\# n \\overline{\\mathbb{C} P^2}\\), \\(\\# m\\mathbb{C} P^2 \\#n\\overline{\\mathbb{C} P^2}\\) or \\(\\# m (S^2 \\times S^2)\\). Given the relation between boundary Lefschetz fibrations and stable generalized complex structures, we conclude that the manifolds \\(S^1 \\times S^3\\# n \\overline{\\mathbb{C} P^2}\\), \\(\\#(2m+1)\\mathbb{C} P^2 \\#n\\overline{\\mathbb{C} P^2}\\) and \\(\\# (2m+1) S^2 \\times S^2\\) admit stable structures whose type change locus has a single component and are the only four-manifolds whose stable structure arise from boundary Lefschetz fibrations over the disc.\n\n\n\n\n\n\n\n\n\nElimination of cusps in dimension 4 and its applications (joint with K. Hayano)\n\n\n\n\n\n(Proc. Lond. Math. Soc., Vol. 113 (2016), No. 5, 674-724) [DOI] [ar\\(\\chi\\)iv]\nAbstract: Several new combinatorial descriptions of closed 4-manifolds have recently been introduced in the study of smooth maps from 4-manifolds to surfaces. These descriptions consist of simple closed curves in a closed, orientable surface and these curves appear as so called vanishing sets of corresponding maps. In the present paper we focus on homotopies canceling pairs of cusps, so called cusp merges. We first discuss the classification problem of such homotopies, showing that there is a one-to-one correspondence between the set of homotopy classes of cusp merges canceling a given pair of cusps and the set of homotopy classes of suitably decorated curves between the cusps. Using our classification, we further give a complete description of the behavior of vanishing sets under cusp merges in terms of mapping class groups of surfaces. As an application, we discuss the uniqueness of surface diagrams, which are combinatorial descriptions of 4-manifolds due to Williams, and give new examples of surface diagrams related with Lefschetz fibrations and Heegaard diagrams.\n\n\n\n\n\n\n\n\n\nOn 4-Manifolds, Folds and Cusps\n\n\n\n\n\n(Pacific J. Math., Vol. 264 (2013), No. 2, 257-306) [DOI] [ar\\(\\chi\\)iv]\nAbstract: We study simple wrinkled fibrations, a variation of the simplified purely wrinkled fibrations introduced by Williams, and their combinatorial description in terms of surface diagrams. We show that simple wrinkled fibrations induce handle decompositions on their total spaces which are very similar to those obtained from Lefschetz fibrations. The handle decompositions turn out to be closely related to surface diagrams and we use this relationship to interpret some cut-and-paste operations on 4-manifolds in terms of surface diagrams. This, in turn, allows us to classify all closed 4-manifolds which admit simple wrinkled fibrations of genus one, the lowest possible fiber genus."
  },
  {
    "objectID": "math.html#teaching",
    "href": "math.html#teaching",
    "title": "Math/Research",
    "section": "Teaching",
    "text": "Teaching"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Mathematician (Ph.D.) with 5+ years of unversity level research and teaching\nSkilled in Python, SQL and cloud technologies\nPassionate about math, coding, data and deep learning\nKnowledgable about digital signal processing, acoustics, and music"
  },
  {
    "objectID": "cv.html#summary",
    "href": "cv.html#summary",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Mathematician (Ph.D.) with 5+ years of unversity level research and teaching\nSkilled in Python, SQL and cloud technologies\nPassionate about math, coding, data and deep learning\nKnowledgable about digital signal processing, acoustics, and music"
  },
  {
    "objectID": "cv.html#research-work-experience",
    "href": "cv.html#research-work-experience",
    "title": "Curriculum Vitae",
    "section": "Research & Work Experience",
    "text": "Research & Work Experience\n\n\n    \n        Assistant Professor\n        (Akademischer Rat auf Zeit)\n        2017 — 2024\n        Bielefeld University — Bielefeld, Germany\n    \n\n\nPlanned and taught over 20 courses with consistenly positive student feedback\nSupervised and evaluated four M.Sc. and three B.Sc. theses\nPublication of the popular monograph \"The Disk Embedding Theorem\",  Behrens et. al., OUP 2021\nResearch projects and scientific publications on the geometry and topology of 3- and 4-dimensional manifolds, focussing on applications of Seiberg-Witten theory\nInternational and national collaborations\n\n\n\n\n\n\n\n    \n        Postdoctoral Researcher\n        \n        2015 — 2017\n        Utrecht University — Utrecht, Netherlands\n    \n\n\nResearch and teaching in mathematics\nPublished several research articles\nCo-supervised B.Sc. thesis\nAttended international conferences\n\n\n\n\n\n\n\n    \n        ERC Postdoctoral Fellow\n        \n        2013 — 2015\n        Alfréd Rényi Institute of Mathamtics — Budapest, Hungary\n    \n\n\nResearch in mathematics\nPublished several papers\nAttended international conferences"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\n\n    \n        Ph.D. in Mathematics\n        (Dr. rer. nat.)\n        05/2014\n        University of Bonn — Bonn, Germany\n    \n\n\nThesis: Surface Diagrams of Smooth 4-Manifolds (supervised by Prof. Dr. Peter Teichner)\nGraduated magna cum laude\nIMPRS Fellow at the Max-Planck-Institute for Mathematics\n\n\n\n\n\n\n\n    \n        M.Sc. in Mathematics\n        (Dipl. Math.)\n        09/2009\n        University of Bonn — Bonn, Germany\n    \n\n\nThesis: The L²-Stokes theorem on certain incomplete manifolds (supervised by Prof. Dr. Matthias Lesch)\nMathematical specialization: Analysis on manifolds, stochastic analysis, algebraic geometry, topology\nChristian-Kuhlemann-Foundation Award for outstanding undergraduate achievements (at Leibniz University Hannover)"
  },
  {
    "objectID": "cv.html#certificates",
    "href": "cv.html#certificates",
    "title": "Curriculum Vitae",
    "section": "Certificates",
    "text": "Certificates\n\n\n    \n        IBM DevOps and Software Engineering\n        \n        in progress\n        IBM / Coursera\n    \n\n\nURLs:\nView Course Details\n\nSubjects Covered: \nDevOps and Agile Methods\nVersion Management with Git and GitHub\nShell scripting in Linux\nCotainerization and Cloud Deployment\n\nTechnologies Used: \nGit\nGitHub\nLinux\nBash\nDocker\nKubernetes\n\n\n\n\n\n\n\n\n    \n        Databases for Data Scientists\n        \n        09/2025\n        University of Pennsylvania / Coursera\n    \n\n\nURLs:\nView Certificate\nView Course Details\n\nSubjects Covered: \nRelational Databases\nNormalization (3NF)\nSQL\nData Warehousing\nNoSQL\n\nTechnologies Used: \nPostgreSQL\n\n\n\n\n\n\n\n\n    \n        AI and Machine Learning Essentials with Python Specialization\n        \n        06/2025\n        University of Pennsylvania / Coursera\n    \n\n\nURLs:\nView Certificate\nView Course Details\n\nSubjects Covered: \nSearch algorithms (informed, uninformed, A*)\nBasic statistics (probability distributions, central limit theorem, confidence intervals)\nMachine learning algorithms (regression, SVMs)\nDeep learning (neural networks)\n\nTechnologies Used: \nPython\nscikit-learn\nPyTorch\n\n\n\n\n\n\n\n\n    \n        IBM Data Science Professional Certificate\n        \n        05/2025\n        IBM / Coursera\n    \n\n\nURLs:\nView Certificate\nView Course Details\n\nSubjects Covered: \nData science methodology (from business requirements to model deployment)\nData collection and preparation\nData analysis and modelling\nModel evaluation and deployment\nMachine learning algorithms (prediction, classification, clustering)\nData visualization (plots, dashboards, presentations)\n\nTechnologies Used: \nPython\nSQL\npandas\nscikit-learn\nmatplotlib\nseaborn\nPlotly Dash\nBeautifulSoup (webscraping)\nREST APIs\n\n\n\n\n\n\n\n\n    \n        Google Cybersecurity Professional Certificate\n        \n        04/2025\n        Google / Coursera\n    \n\n\nURLs:\nView Certificate\nView Course Details\n\nSubjects Covered: \nNIST frameworks\nOWASP security principles\nNetwork models (TCP/IP, OSI)\nNetwork protocols (TCP, UDP, HTTP(S))\nNetwork monitoring\nLog analysis\nThreat detection\nIncident response\nIdentity and access management\nEncryption\nAuthentication\nAuthorization\nCloud security basics\n\nTechnologies Used: \nLinux\nPython\nSQL\nWireshark\ntcpdump\n\n\n\n\n\n\n\n\n    \n        Python 3 Programming Specialization\n        \n        02/2025\n        University of Michigan / Coursera\n    \n\n\nURLs:\nView Certificate\nView Course Details\n\nSubjects Covered: \nProgramming fundamentals\nObject oriented programming\nData collection and processing\n\nTechnologies Used: \nPython\nJupyterLab\nHTTP protocol\nREST APIs"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Talking to AMV FRITZ!Box Routers in Python\n\n\n\nPython\n\nsb4dfritz\n\n\n\n\n\n\n\n\n\nSep 10, 2025\n\n\nStefan Behrens\n\n\n\n\n\n\n\n\n\n\n\n\nFixing a Database Using Machine Learning\n\n\n\nPython\n\nSQL\n\nData Analysis\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nSep 9, 2025\n\n\nStefan Behrens\n\n\n\n\n\n\n\n\n\n\n\n\nReloading imports in Jupyter Notebooks\n\n\n\nPython\n\nJupyter\n\nLittle Tricks\n\n\n\n\n\n\n\n\n\nSep 1, 2025\n\n\nStefan Behrens\n\n\n\n\n\n\n\n\n\n\n\n\nLoading “formattble” strings from plain text files\n\n\n\nLittle Tricks\n\nPython\n\n\n\n\n\n\n\n\n\nAug 22, 2025\n\n\nStefan Behrens\n\n\n\n\n\n\n\n\n\n\n\n\nThat relative imports hack for Python scripts\n\n\n\nPython\n\nLittle Tricks\n\n\n\n\n\n\n\n\n\nAug 22, 2025\n\n\nStefan Behrens\n\n\n\n\n\n\n\n\n\n\n\n\nMade a website using Quarto\n\n\n\nWebdesign\n\nQuarto\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\nStefan Behrens\n\n\n\n\n\n\n\n\n\n\n\n\nCustom LaTeX Commands in Quarto Pages\n\n\n\nQuarto\n\nLaTeX\n\nJavaScript\n\n\n\n\n\n\n\n\n\nAug 9, 2025\n\n\nStefan Behrens\n\n\n\n\n\n\n\n\n\n\n\n\nCustom LaTeX Commands in Jupyter Notebooks\n\n\n\nJupyter\n\nLaTeX\n\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\nStefan Behrens\n\n\n\n\n\n\n\n\n\n\n\n\nChanging the default figure size in matplotlib Pyplot\n\n\n\nPython\n\nmatplotlib\n\nLittle Tricks\n\n\n\n\n\n\n\n\n\nAug 1, 2025\n\n\nStefan Behrens\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Licensing for This Repository",
    "section": "",
    "text": "The written content, images, and other non-code materials in this repository are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License.\nYou are free to: - Share — copy and redistribute the material in any medium or format - Adapt — remix, transform, and build upon the material for any purpose, even commercially\nUnder the following terms: - Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made.\nFull license text: https://creativecommons.org/licenses/by/4.0/\n\n\n\n\nThe code portions of this repository are licensed under the MIT License:\nMIT License\nCopyright (c) [YEAR] [YOUR NAME]\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "LICENSE.html#website-content-text-figures-media",
    "href": "LICENSE.html#website-content-text-figures-media",
    "title": "Licensing for This Repository",
    "section": "",
    "text": "The written content, images, and other non-code materials in this repository are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License.\nYou are free to: - Share — copy and redistribute the material in any medium or format - Adapt — remix, transform, and build upon the material for any purpose, even commercially\nUnder the following terms: - Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made.\nFull license text: https://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "LICENSE.html#website-code-quarto-configuration-scripts-templates-styles",
    "href": "LICENSE.html#website-code-quarto-configuration-scripts-templates-styles",
    "title": "Licensing for This Repository",
    "section": "",
    "text": "The code portions of this repository are licensed under the MIT License:\nMIT License\nCopyright (c) [YEAR] [YOUR NAME]\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "posts/latex_macros_in_quarto_pages.html",
    "href": "posts/latex_macros_in_quarto_pages.html",
    "title": "Custom LaTeX Commands in Quarto Pages",
    "section": "",
    "text": "This is companion post to this one where I wrote about using custom LaTeX commands in Jupyter notebooks.\nI’ll briefly go into the backstory again. I’ve used LaTeX for over a decade as a mathematician and I’ve gotten used to a certain set of custom LaTeX commands that I assembled into a custom LaTeX package. Let’s call it mycommands.sty for demonstrational purposes. Recently, I’ve been making more frequent use of Jupyter Notebooks and also Quarto - the latter mostly for this homepage/blog combo. Unfortunately, it doesn’t seem to be possible at this point to load LaTeX packages in Jupyter or Quarto. So I was looking for ways to have my favorite commands available.\nIdeally, I wanted to have a solution that mimics the behavior of \\usepackage{mycommands} in Quarto. Before I get into Quart and it’s markup syntax, here’s an excerpt from my custom LaTeX package:\nBy the way, I’m using \\providecommand instead of \\newcommand to avoid conflicts.\nLike LaTeX, Quarto pages are plain text files and only the extension .qmd makes them recognizable as Quarto files. Quarto has its own markup syntax and, while LaTeX support is built in, the LaTeX code is generally not handled by a full LaTeX distribution. Instead it is rendered uning engines such as MathJax or KaTeX which only provide a limited set of commands. Luckily, it is possible to define custom commands in a Quarto page."
  },
  {
    "objectID": "posts/latex_macros_in_quarto_pages.html#simple-latex-macros-in-quarto-files",
    "href": "posts/latex_macros_in_quarto_pages.html#simple-latex-macros-in-quarto-files",
    "title": "Custom LaTeX Commands in Quarto Pages",
    "section": "Simple LaTeX Macros in Quarto Files",
    "text": "Simple LaTeX Macros in Quarto Files\nSay we want to have some fun with the quaternions and want to write them as \\(\\mathbb{H}\\). If the symbol only appears ones or twice, writing \\mathbb{H} would arguably be the best solution. But it’s used constantly, it would be much better to declare an abbreviation that’s easier to type and also easier on the eyes.\nIn LaTeX I would a custom command \\newcommand{\\HH}{\\mathbb{H}} so that I only have to type \\HH to produce \\(\\mathbb{H}\\). In Quarto files, you can essentially do the same thing, you just have to wrap it in a math environment like this:\n`$\\newcommand{\\HH}{\\mathbb{H}}$`\n\\(\\newcommand{\\HH}{H}\\)\nYou can put this anywhere in your Quarto page, the only restriction is that the code needs to have an empty line above and below.\nAs in LaTeX, it is good practice to declare all your custom commands in the beginning of the file before the main text. The cleanest way to do this in a Quarto file is to begin the file like this:\n---\nauthor: Som Won\ntitle: LaTeX Custom Commands in Quarto Pages\n---\n:::{.hidden}\n$$\n\\newcommand{\\HH}{\\mathbb{H}}\n\\newcommand{\\anothercommand}{\\mathfrak{AnotherCommand}}\n$$\n:::\nWhile the above solution is easy enough, it has some severe drawbacks. First, it only works on a per file basis, which is not very convenient. Second, the :::{.hidden} ... ::: wrapper is supposed to make sure nothing inside gets rendered. Well, it doesn’t, really…\nWarning! At first, I naively copied the original code from mycommands.sty into a :::{.hidden} $$ ...$$  ::: wrapper. I put this code in a separate .qmd file and made sure the code get included in every page using Quarto’s include features. This worked fine for pages and blog posts, but for some reason the preview text on the blog home page contained this garbage:\n$ % Numbers % natural numbers % integers % rational numbers % real numbers % complex numbers % quaternions\nApparently, the problem is related to LaTeX-style comments in the mycommands.sty code. So I removed the comments, but the blog home page still contains $ as an artefact (or $$, I forgot).\nBottom line, this method doesn’t quite cut it for me. Luckily, I found a better way that I’ll explain next."
  },
  {
    "objectID": "posts/latex_macros_in_quarto_pages.html#loading-latex-macros-from-a-javascript-.js-file",
    "href": "posts/latex_macros_in_quarto_pages.html#loading-latex-macros-from-a-javascript-.js-file",
    "title": "Custom LaTeX Commands in Quarto Pages",
    "section": "Loading LaTeX Macros from a JavaScript .js file",
    "text": "Loading LaTeX Macros from a JavaScript .js file\nAfter trying a few things out, I settled on the following solution. The first thing to understand is that Quarto uses MathJax as a default rendering engine for LaTeX in websites. If you want, you can override this and use something like KaTeX instead (which is what VS Code uses for rendering LaTeX in Jupyter Notebooks).\nWith this understood, the main task is to tell MathJax and/or KaTeX (or what ever else you want to use) about your custom commands. Here’s what I did for this website:\n\nInclude the LaTeX commands in a JavaScript file /_tools/sb4dlatex.js (code below).\nCreate a one-line HTML file /_tools/sb4dlatex.html referencing the script file (code below).\nAdd the following line to _quarto.yml:\n\nformat:\n  html:\n    include-in-header: _tools/sb4dlatex.html\nNow the promised code followed by further explanation. First, the HTML bit:\n\n\n/_tools/sb4dlatex.html\n\n&lt;script src=\"/_tools/sb4dlatex.js\"&gt;&lt;/script&gt;\n\nThe line include-in-header: _tools/sb4dlatex.html in _quarto.yml makes sure that the line above gets included in every page header. Finally, the JavaScript bit:\n\n\n/_tools/sb4dlatex.js\n\n// Define custom LaTeX commands\nconst custom_latex_commands = {\n    // Numbers\n    N: \"\\\\mathbb{N}\",  // natural numbers\n    Z: \"\\\\mathbb{Z}\",  // integers\n    Q: \"\\\\mathbb{Q}\",  // rational numbers\n    R: \"\\\\mathbb{R}\",  // real numbers\n    C: \"\\\\mathbb{C}\",  // complex numbers\n    HH: \"\\\\mathbb{H}\", // quaternions\n    // Math font styles\n    mbb: [\"\\\\mathbb{#1}\", 1],\n    mc: [\"\\\\mathcal{#1}\", 1],\n    mf: [\"\\\\mathfrak{#1}\", 1],\n    // Math font decoration\n    bs: [\"\\\\boldsymbol{#1}\", 1],\n    wt: [\"\\\\widetilde{#1}\", 1],\n    // Arrows\n    xra: [\"\\\\xrightarrow{#1}\", 1],\n    // Useful things\n    inv: \"^{-1}\", // inverses\n    del: \"\\\\partial\", // partial derivatives\n    // Special notation\n    TT: \"\\\\mathbb{T}\", // circle group\n};\n// Make custom LaTeX commands available for MathJax\nwindow.MathJax = {\n    tex: {\n        macros: custom_latex_commands\n    }\n};\n// Make custom LaTeX commands available for KaTeX\nwindow.katexMacros = custom_latex_commands;\n\nNot too bad, right? While the syntax is not entirely self-explanatory, comparing it with the LaTeX code in mycommands.sty should give you an idea how it works. Let’s look at two commands in detail.\nLaTex:\n\\providecommand{\\HH}{\\mathbb{H}} % quaternions\n\\providecommand{\\mc}[1]{\\mathcal{#1}}\nJavaScript (in combination with KaTeX for brevity):\nconst latex_commands = {\n  HH: \"\\\\mathbb{H}\", // quaternions\n  mc: [\"\\\\mathcal{#1}\", 1],\n};\nwindow.katexMacros = latex_commands;\nFirst, let’s think about what’s happening here. \\providecommand{\\HH}{\\mathbb{H}} tells your LaTeX compiler to treat the string \\HH as \\mathbb{H}. Abstractly, we can think of these two strings as a key-value pair. Observe that \\providecommand{&lt;key&gt;}{&lt;value&gt;} is called individually for every key-value pair. In contrast, the Javascript solution records all key-value pairs in a dictionary (aka JavaScript object) and then passes that dictionary to the LaTeX interpreter (using window.katexMacros = latex_commands; for KaTeX).\nSecond, the syntactic differences are mainly the backslashes, the double quotes, and the handling of arguments for \\mc{&lt;arg&gt;}. Note that the dictionary values (i.e. the LaTeX commands to be replaced) are written as strings in double quotes and the single backslash \\ needs to be escaped as \\\\. The dictioary keys (i.e. the designated LaTeX shortuts), however, are written in plain without quotes or backslashes. Lastly, the add-arguments syntax should be reasonably self-explanatory. The number of arguments needs to be specified, albeit in a slightly different place as in LaTeX, and the arguments are called as #1, #2, … inside the LaTeX string.\nI’ll leave it at that for the moment. You may rightfully ask: “But what about optional arguments, possibly with default values?” I’m aware of the omission. I’ll get back to this later when I actually need such a construct. For now, I’ll keep things simple."
  },
  {
    "objectID": "posts/pyplot_figsize_default.html",
    "href": "posts/pyplot_figsize_default.html",
    "title": "Changing the default figure size in matplotlib Pyplot",
    "section": "",
    "text": "I often use matplotlib’s pyplot for quick plots while I’m trying things out in Jupyter Notebooks. I’m rarely ever happy with the default size of the plots and it gets annoying to change it manually for every single plot. Luckily, there’s an easy way to change this on a per document basis.\nAssuming that pyplot is imported as plt, the default figure size is store in a dictionary entry plt.rcParams['figure.figsize']. Changing its value sets the default figure size for the current document.\n\nimport matplotlib.pyplot as plt\n\nfactory_default_figsize = plt.rcParams['figure.figsize']\nprint(\"Factory setting for figsize:\", factory_default_figsize)\n\nFactory setting for figsize: [6.4, 4.8]\n\n\n\n## Example 1: Factory default figsize\nimport numpy as np\n\nx = np.linspace(0,10,1000)\ny = np.sin(x**2)\n\nplt.plot(x, y)\n\n\n\n\n\n\n\n\n\n##  Example 2: Custom default figsize\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# set a custom default figsize\ncustom_default_figsize = (15,5)\nplt.rcParams['figure.figsize'] = custom_default_figsize\n\nx = np.linspace(0,10,1000)\ny = np.sin(x**2)\n\nplt.plot(x, y)\n\n\n\n\n\n\n\n\nLastly, it is worth mentioning that the dictionary plt.rcParams contains many other pyplot default setting (if not all of them). As I understand it, it is populated with the factory defaults everytime pyplot is being imported. Importantly, changing plt.rcParams['figure.figsize'] does not overwrite the factory settings! They are safely stored somewhere else. So doing this in one document does not affect any other."
  },
  {
    "objectID": "posts/placeholders_in_text_files.html",
    "href": "posts/placeholders_in_text_files.html",
    "title": "Loading “formattble” strings from plain text files",
    "section": "",
    "text": "I was recently playing around with generating HTML or LaTeX files using Python. For that purpose, it’s often conventient to use “formattable” strings with placeholders (eg. {stuff:s}) as code templates that can be filled with content. Sometimes it’s also a good idea to outsource these templates as plain text in auxiliary files (e.g. .html or .tex files). I’m going to demonstrate to methods to do this."
  },
  {
    "objectID": "posts/placeholders_in_text_files.html#standard-string-formatting",
    "href": "posts/placeholders_in_text_files.html#standard-string-formatting",
    "title": "Loading “formattble” strings from plain text files",
    "section": "Standard string formatting",
    "text": "Standard string formatting\nBelow I’ve put some placeholder of the form {stuff:s} in an HTML file.\n\n# Load HTML file with placeholders and print\ntemplate = \"./templates/test_format.html\"\nwith open(template, \"r\") as file:\n    html_template = file.read()\nprint(\"HTML with placeholders:\")\nprint(\"-\"*20)\nprint(html_template)\n\nHTML with placeholders:\n--------------------\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;{page_title:s}&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;{h1:s}&lt;/h1&gt;\n    &lt;p&gt;{text:s}&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\nSince the placeholders are now in a string, standard string formatting is available:\n\n# Fill placeholders and print again\nprint(\"HTML after filling placeholders:\")\nprint(\"-\"*20)\nhtml = html_template.format(\n    page_title=\"Title\",\n    h1=\"Headline\",\n    text=\"This is some text.\"\n)\nprint(html)\n\nHTML after filling placeholders:\n--------------------\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Headline&lt;/h1&gt;\n    &lt;p&gt;This is some text.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/placeholders_in_text_files.html#template-stings",
    "href": "posts/placeholders_in_text_files.html#template-stings",
    "title": "Loading “formattble” strings from plain text files",
    "section": "Template Stings",
    "text": "Template Stings\nThe same works with template strings using placeholders of the form $stuff. To make use of these, the Template class from the string library is needed.\n\n# Import the `Template` class from the string library\nfrom string import Template\n\n# Load HTML file with placeholders and print\ntemplate = \"./templates/test_Template.html\"\nwith open(template, \"r\") as file:\n    html_template = file.read()\nprint(\"HTML with placeholders:\")\nprint(\"-\"*20)\nprint(html_template)\n\nHTML with placeholders:\n--------------------\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;$page_title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;$h1&lt;/h1&gt;\n    &lt;p&gt;$text&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\nThe placeholder can be filled by creating a Template object from the string and then using the metho .substitute(...).\n\n# Create \"template string\"\nhtml_template = Template(html_template)\n\n# Fill placeholders and print again\nprint(\"HTML after filling placeholders:\")\nprint(\"-\"*20)\nhtml = html_template.substitute(\n    page_title=\"Title\",\n    h1=\"Headline\",\n    text=\"This is some text.\"\n)\nprint(html)\n\nHTML after filling placeholders:\n--------------------\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Headline&lt;/h1&gt;\n    &lt;p&gt;This is some text.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/drafts/placeholders_in_text_files.html",
    "href": "posts/drafts/placeholders_in_text_files.html",
    "title": "Loading “formattble” strings from plain text files",
    "section": "",
    "text": "I was recently playing around with generating HTML or LaTeX files using Python. For that purpose, it’s often conventient to use “formattable” with placeholders (eg. {stuff:s}) as code templates that can be filled with content that might. Sometime it’s also a good idea to outsource these “formattable” strings to auxiliary plain text files (e.g. .html or .tex files). I’m going to demonstrate to methods to do this."
  },
  {
    "objectID": "posts/drafts/placeholders_in_text_files.html#standard-string-formatting",
    "href": "posts/drafts/placeholders_in_text_files.html#standard-string-formatting",
    "title": "Loading “formattble” strings from plain text files",
    "section": "Standard string formatting",
    "text": "Standard string formatting\nBelow I’ve put some placeholder of the form {stuff:s} in an HTML file.\n\n# Load HTML file with placeholders and print\ntemplate = \"./templates/test_format.html\"\nwith open(template, \"r\") as file:\n    html_template = file.read()\nprint(\"HTML with placeholders:\")\nprint(\"-\"*20)\nprint(html_template)\n\nHTML with placeholders:\n--------------------\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;{page_title:s}&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;{h1:s}&lt;/h1&gt;\n    &lt;p&gt;{text:s}&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\nSince the placeholders are now in a string, standard string formatting is available:\n\n# Fill placeholders and print again\nprint(\"HTML after filling placeholders:\")\nprint(\"-\"*20)\nhtml = html_template.format(\n    page_title=\"Title\",\n    h1=\"Headline\",\n    text=\"This is some text.\"\n)\nprint(html)\n\nHTML after filling placeholders:\n--------------------\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Headline&lt;/h1&gt;\n    &lt;p&gt;This is some text.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/drafts/placeholders_in_text_files.html#template-stings",
    "href": "posts/drafts/placeholders_in_text_files.html#template-stings",
    "title": "Loading “formattble” strings from plain text files",
    "section": "Template Stings",
    "text": "Template Stings\nThe same works with template strings using placeholders of the form $stuff. To make use of these, the Template class from the string library is needed.\n\n# Import the `Template` class from the string library\nfrom string import Template\n\n# Load HTML file with placeholders and print\ntemplate = \"./templates/test_Template.html\"\nwith open(template, \"r\") as file:\n    html_template = file.read()\nprint(\"HTML with placeholders:\")\nprint(\"-\"*20)\nprint(html_template)\n\nHTML with placeholders:\n--------------------\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;$page_title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;$h1&lt;/h1&gt;\n    &lt;p&gt;$text&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\nThe placeholder can be filled by creating a Template object from the string and then using the metho .substitute(...).\n\n# Create \"template string\"\nhtml_template = Template(html_template)\n\n# Fill placeholders and print again\nprint(\"HTML after filling placeholders:\")\nprint(\"-\"*20)\nhtml = html_template.substitute(\n    page_title=\"Title\",\n    h1=\"Headline\",\n    text=\"This is some text.\"\n)\nprint(html)\n\nHTML after filling placeholders:\n--------------------\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Headline&lt;/h1&gt;\n    &lt;p&gt;This is some text.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/drafts/placeholders_in_text_files.html#a-latex-example",
    "href": "posts/drafts/placeholders_in_text_files.html#a-latex-example",
    "title": "Loading “formattble” strings from plain text files",
    "section": "A LaTeX example",
    "text": "A LaTeX example\nTemplate strings are particularly useful when working with LaTeX code, which usually contains a lot of backslashes \\ and curly braces {...}. The curly braces are particularly annoying.\nHere’s a simple example, again, first with placeholders.\n\n# Import the `Template` class from the string library\nfrom string import Template\n\n# Load LaTeX file with placeholders and print\ntemplate = \"./templates/test_Template.tex\"\nwith open(template, \"r\") as file:\n    latex_template = file.read()\nprint(\"LaTeX with placeholders:\")\nprint(\"-\"*20)\nprint(latex_template)\n\nLaTeX with placeholders:\n--------------------\n\\documentclass{article}\n\n\\author{Author}\n\\title{Title}\n\\date{\\today}\n\n\\begin{document}\n\n\\section{$SECTION}\n\n$TEXT\n\n\\end{document}\n\n\nAnd now with content:\n\n# Create \"template string\"\nlatex_template = Template(latex_template)\n\n# Fill placeholders and print again\nprint(\"LaTeX after filling placeholders:\")\nprint(\"-\"*20)\nlatex = latex_template.substitute(\n    SECTION=\"Introduction\",\n    TEXT= \"This is some text.\"\n)\nprint(latex)\n\nLaTeX after filling placeholders:\n--------------------\n\\documentclass{article}\n\n\\author{Author}\n\\title{Title}\n\\date{\\today}\n\n\\begin{document}\n\n\\section{Introduction}\n\nThis is some text.\n\n\\end{document}\n\n\n\nTrouble with standard string formatting\nFinally, here’s a quick demonstration that standard string formatting does not work in this example. The problem is that Python cannot distinguiing the placeholder {stuff:s} from the many other things wrapped in curly braces. Every pair {...} looks like a placeholder.\nIn the example below, the interpreter treats {article} as in \\documentclass{article} is a placeholder with key \"article\" and reports a key error.\n\n# Load LaTeX file with placeholders and print\ntemplate = \"./templates/test_format.tex\"\nwith open(template, \"r\") as file:\n    latex_template = file.read()\nprint(\"LaTeX with placeholders:\")\nprint(\"-\"*20)\nprint(latex_template)\nprint(\"\\n\")\n\n# Fill placeholders and print again\nprint(\"LaTeX after filling placeholders:\")\nprint(\"-\"*20)\nlatex = latex_template.format(\n    SECTION=\"Introduction\",\n    TEXT= \"This is some text.\"\n)\nprint(latex)\n\nLaTeX with placeholders:\n--------------------\n\\documentclass{article}\n\n\\author{Author}\n\\title{Title}\n\\date{\\today}\n\n\\begin{document}\n\n\\section{{SECTION:s}}\n\n{TEXT:s}\n\n\\end{document}\n\n\nLaTeX after filling placeholders:\n--------------------\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n~\\AppData\\Local\\Temp\\ipykernel_8280\\2517583926.py in &lt;cell line: 0&gt;()\n     11 print(\"LaTeX after filling placeholders:\")\n     12 print(\"-\"*20)\n---&gt; 13 latex = latex_template.format(\n     14     SECTION=\"Introduction\",\n     15     TEXT= \"This is some text.\"\n\nKeyError: 'article'"
  },
  {
    "objectID": "posts/drafts/relative_imports_hack.html",
    "href": "posts/drafts/relative_imports_hack.html",
    "title": "That relative imports hack for Python scripts",
    "section": "",
    "text": "I need to write this down so I don’t forget it again. I’ve been struggling with “relative imports” in Python projects in VS Code. Here’s the problem."
  },
  {
    "objectID": "posts/drafts/relative_imports_hack.html#the-problem",
    "href": "posts/drafts/relative_imports_hack.html#the-problem",
    "title": "That relative imports hack for Python scripts",
    "section": "The problem",
    "text": "The problem\nSay I’m working on a Python project in VS Code. The project is in early stages and my file/folder structure looks something like this:\nproject/\n    my_package/\n        __init__.py\n        my_module1.py\n        my_module2.py\n    experimental/\n        experiment1.py\n        experiment2.py\n    main.py\nThe idea is to put some of the code required for main.py into a custom package my_package. While working on the package, I might want to try a few things out. That’s what the scripts in experimental/ are for. I’m keeping them in a sub-folder so I can easily .gitignore them, and also to keep the main project folder clean.\nObviously, I want to import things my_package into the experimental scripts. Naively, I tried this:\n\n\nexperiment1.py\n\nimport ..my_package\nfrom ..my_package import my_module1\n\nWhile typing this, VS Code does not complain about this and it feels like it should work. After all, .. indicates passage to the parent directory, that is, the project directory where my_package lives. However, when I hit the “play button” in the VS Code tab for experiment1.py, I get this error in the terminal:\nImportError: attempted relative import with no known parent package\nI’m not going to get into why this doesn’t work. Instead I’ll go straight to a quick and dirty solution."
  },
  {
    "objectID": "posts/drafts/relative_imports_hack.html#a-quick-and-dirty-solution",
    "href": "posts/drafts/relative_imports_hack.html#a-quick-and-dirty-solution",
    "title": "That relative imports hack for Python scripts",
    "section": "A quick-and-dirty solution",
    "text": "A quick-and-dirty solution\nOne solution is to manually instruct the Python interpreter where to look for packages and modules. In this case it needs to look in the project root directory while it runs the script in the experimental/ folder. Here’s a way to do this:\n\n\nexperiment1.py\n\n# Import the needed tools\nimport sys\nfrom pathlib import Path\n# Add project root to `sys.path` and...\nsys.path.append(str(Path(__file__).resolve().parents[1]))\n\n# ...and proceed as before without `..`\nimport my_package\nfrom my_package import my_module1\n\nI must have looked this up three to five times now. I’m still not sure if I’ll be able to remember it next time. But at least I know where to look…\nAnd yes, I’ve read that this is not good practice and should never be used in production code. But it seems to be fine for my purely experimental purposes."
  },
  {
    "objectID": "posts/relative_imports_hack.html",
    "href": "posts/relative_imports_hack.html",
    "title": "That relative imports hack for Python scripts",
    "section": "",
    "text": "I need to write this down so I don’t forget it again. I’ve been struggling with “relative imports” in Python projects in VS Code. Here’s the problem."
  },
  {
    "objectID": "posts/relative_imports_hack.html#the-problem",
    "href": "posts/relative_imports_hack.html#the-problem",
    "title": "That relative imports hack for Python scripts",
    "section": "The problem",
    "text": "The problem\nSay I’m working on a Python project in VS Code. The project is in early stages and my file/folder structure looks something like this:\nproject/\n    my_package/\n        __init__.py\n        my_module1.py\n        my_module2.py\n    experimental/\n        experiment1.py\n        experiment2.py\n    main.py\nThe idea is to put some of the code required for main.py into a custom package my_package. While working on the package, I might want to try a few things out. That’s what the scripts in experimental/ are there for. I’m keeping them in a sub-folder so I can easily .gitignore them, and also to keep the main project folder clean.\nObviously, I want to import things from my_package into the experimental scripts. Naively, I tried this:\n\n\nexperiment1.py (not working!)\n\nimport ..my_package\nfrom ..my_package import my_module1\n\nWhile typing this, VS Code does not complain and it feels like it should work. After all, the double dorts .. indicate passage to the parent directory, which is the project root directory in this case and where my_package lives. However, when I hit the “play button” in the VS Code tab for experiment1.py, say, I get this error in the terminal:\nImportError: attempted relative import with no known parent package\nI’m not going to get into why this doesn’t work. Instead I’ll go straight to a quick and dirty solution."
  },
  {
    "objectID": "posts/relative_imports_hack.html#a-quick-and-dirty-solution",
    "href": "posts/relative_imports_hack.html#a-quick-and-dirty-solution",
    "title": "That relative imports hack for Python scripts",
    "section": "A quick-and-dirty solution",
    "text": "A quick-and-dirty solution\nOne solution is to manually instruct the Python interpreter where to look for packages and modules. In this case it needs to look in the project root directory while it’s running the script in the experimental/ folder. Here’s a way to do this:\n\n\nexperiment1.py (working!)\n\n# Import the needed tools\nimport sys\nfrom pathlib import Path\n# Add project root to `sys.path` and...\nsys.path.append(str(Path(__file__).resolve().parents[1]))\n\n# ...and proceed as before without `..`\nimport my_package\nfrom my_package import my_module1\n\nI must have looked this up three to five times now. I’m still not sure if I’ll be able to remember it next time. But at least I know where to look now…\nAnd yes, I’ve read that this is not good practice and should never be used in production code. But it seems to be fine for my experimental purposes."
  },
  {
    "objectID": "posts/placeholders_in_text_files.html#a-latex-example",
    "href": "posts/placeholders_in_text_files.html#a-latex-example",
    "title": "Loading “formattble” strings from plain text files",
    "section": "A LaTeX example",
    "text": "A LaTeX example\nTemplate strings are particularly useful when working with LaTeX code, which usually contains a lot of backslashes \\ and curly braces {...}. The curly braces are particularly annoying.\nHere’s a simple example, again, first with placeholders.\n\n# Import the `Template` class from the string library\nfrom string import Template\n\n# Load LaTeX file with placeholders and print\ntemplate = \"./templates/test_Template.tex\"\nwith open(template, \"r\") as file:\n    latex_template = file.read()\nprint(\"LaTeX with placeholders:\")\nprint(\"-\"*20)\nprint(latex_template)\n\nLaTeX with placeholders:\n--------------------\n\\documentclass{article}\n\n\\author{Author}\n\\title{Title}\n\\date{\\today}\n\n\\begin{document}\n\n\\section{$SECTION}\n\n$TEXT\n\n\\end{document}\n\n\nAnd now with content:\n\n# Create \"template string\"\nlatex_template = Template(latex_template)\n\n# Fill placeholders and print again\nprint(\"LaTeX after filling placeholders:\")\nprint(\"-\"*20)\nlatex = latex_template.substitute(\n    SECTION=\"Introduction\",\n    TEXT= \"This is some text.\"\n)\nprint(latex)\n\nLaTeX after filling placeholders:\n--------------------\n\\documentclass{article}\n\n\\author{Author}\n\\title{Title}\n\\date{\\today}\n\n\\begin{document}\n\n\\section{Introduction}\n\nThis is some text.\n\n\\end{document}\n\n\n\nTrouble with standard string formatting\nFinally, here’s a quick demonstration that standard string formatting does not work in this example. The problem is that Python cannot distinguiing the actual placeholders {stuff:s} from the many other things wrapped in curly braces. Every pair {...} looks like a placeholder.\nIn the example below, the interpreter treats {article} as in \\documentclass{article} is a placeholder with key \"article\" and reports a key error.\n\n# Load LaTeX file with placeholders and print\ntemplate = \"./templates/test_format.tex\"\nwith open(template, \"r\") as file:\n    latex_template = file.read()\nprint(\"LaTeX with placeholders:\")\nprint(\"-\"*20)\nprint(latex_template)\nprint(\"\\n\")\n\n# Fill placeholders and print again\nprint(\"LaTeX after filling placeholders:\")\nprint(\"-\"*20)\nlatex = latex_template.format(\n    SECTION=\"Introduction\",\n    TEXT= \"This is some text.\"\n)\nprint(latex)\n\nLaTeX with placeholders:\n--------------------\n\\documentclass{article}\n\n\\author{Author}\n\\title{Title}\n\\date{\\today}\n\n\\begin{document}\n\n\\section{{SECTION:s}}\n\n{TEXT:s}\n\n\\end{document}\n\n\nLaTeX after filling placeholders:\n--------------------\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n~\\AppData\\Local\\Temp\\ipykernel_8280\\2517583926.py in &lt;cell line: 0&gt;()\n     11 print(\"LaTeX after filling placeholders:\")\n     12 print(\"-\"*20)\n---&gt; 13 latex = latex_template.format(\n     14     SECTION=\"Introduction\",\n     15     TEXT= \"This is some text.\"\n\nKeyError: 'article'"
  },
  {
    "objectID": "posts/reloading_packages.html",
    "href": "posts/reloading_packages.html",
    "title": "Reloading imports in Jupyter Notebooks",
    "section": "",
    "text": "This is another short post mostly written for me to remember something. When working on a Python library, I like to have a Jupyter Notebook for testing. However, by default, a Jupyter Notebook only runs every import once. So any imported module that’s actively being developed remains in the state in which it was first loaded until a forced reload.\nOne way to do a forced reload is to restart the kernel. But this is often inconvenient. A more convenient way is to use the importlib module as follows:\n# import the importlib library first\nimport importlib\n# next import your module\nimport my_module\n# force the module to reload each time the cell runs\nimportlib.reload(my_module)\n# only now laod specific functions from that the module\nfrom my_module import some_function\nImportant: Even if you only want the last line of code, you have to include the first four (ignoring comments). Reloading individual functions does not work this way, that is, importlib.reload(some_function) produces an error."
  },
  {
    "objectID": "posts/fixing_my_fitnotes_db.html",
    "href": "posts/fixing_my_fitnotes_db.html",
    "title": "Fixing a Database Using Machine Learning",
    "section": "",
    "text": "I use a combination of SQL, data analysis, and machine learning to fix faulty entries in the database of my workout tracking app."
  },
  {
    "objectID": "posts/fixing_my_fitnotes_db.html#the-problem-and-the-approach",
    "href": "posts/fixing_my_fitnotes_db.html#the-problem-and-the-approach",
    "title": "Fixing a Database Using Machine Learning",
    "section": "The Problem and the Approach",
    "text": "The Problem and the Approach\nI’ve been using the FitNotes app to track my workouts since 2018. Among other things, I’ve been doing the bar bell classics: bench press, squat, deadlift, and overhead press. Once I got the hang of the exercises, I started doing a few warm-up sets with lower weights followed by the regular sets. The problem is that I eventually decided to track the warm-up sets separately, but didn’t do so in the beginning. The resulting inconsistency of my work-out log has been bothering me ever since.\nMore precisely, in the beginning all deadlift sets were tracked under the exercise label “Deadlift”. After a while I introduced another label “Deadlift (Warm Up)” and the same goes for the other three exercises. Altogether, I have about 5000 tracked sets for the four exercises of which roughly 1000 predate the introduction of the “… (Warm Up)” labels - the precise numbers will be determined later.\nLuckily, FitNotes provides a convenient backup and restore feature. A full backup comes in the form of an SQLite3 file, which is reasonably easy to work with. The task is to find the database entries representing the falsely labeled warm-up sets, and to assign the correct labels.\nWhile the large number of potentially falsely categorized sets essentially rules out a manual approach, the even larger number of properly categorized sets suggests a machine learning approach: train a suitable classifier model on the good data and use it to identify the bad data.\nA preview of the main function should give a good idea of the things to come:\ndef fix_db_file(db_file):\n    # get the data from the database file\n    data = load_data(db_file)\n    # prepare the data\n    prepared_data = prepare_data(data)\n    # make prediction which early sets are warm-ups\n    predicted_data = predict_warm_ups(prepared_data)\n    # determine the \"bad\" set IDs\n    bad_set_ids = get_bad_ids(predicted_data)\n    # update database file\n    update_db_file(db_file, bad_set_ids)\nThe following packages will be used in the implementation:\n\n# database and data handling\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n# classical machine learning\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\n# deep learning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n# visualization\nimport matplotlib.pyplot as plt\n# file operations\nfrom datetime import datetime\nimport shutil"
  },
  {
    "objectID": "posts/fixing_my_fitnotes_db.html#step-1-loading-the-data",
    "href": "posts/fixing_my_fitnotes_db.html#step-1-loading-the-data",
    "title": "Fixing a Database Using Machine Learning",
    "section": "Step 1: Loading the Data",
    "text": "Step 1: Loading the Data\nThe FitNotes backups come in the form of files named like FitNotes_Backup_2025_08_19_11_53_20.fitnotes, for example. As mentioned, these are just SQLite3 files which are reasonably easy to work with (arguably as easy as it gets with SQL databases). To get started, I opened the file in DB Browser for SQLite which is great for exploring SQLite databases and drafting queries. In this case, I found that there are 24 tables, but only two of them are relevant:\n\nexercise: contains the information about the different exercises. The relevant columns are\n\n_id: a numerical primary key\nname: the exercise names\n\ntraining_log: This contains the training logs with one row for each recorded set. The relevant columns are:\n\n_id: again, a numerical primary key\ndate: the date in the format YYYY-MM-DD\nmetric_weight: the weight for the set in kilograms (kg)\nreps: the number of repitions in the set\nexercise_id: a foreign key referencing the exercise with the corresponding _id in the exercise table\n\n\nWith this understood, it’s time to run some queries. I’m opting for a combination of sqlite3 and pandas to have the query results available as dataframes and ready for further processing.\n\n# Specify path of the FitNotes backup file\nFITNOTES_BACKUP = \"FitNotes_Backup_2025_08_19_11_53_20.fitnotes\"\n\n# Use the following function to run queries\ndef run_query(query, db=FITNOTES_BACKUP):\n    \"\"\"Run SQL query and return results as dataframe.\"\"\"\n    with sqlite3.connect(db) as conn:\n        results = pd.read_sql(sql=query, con=conn)\n    return results\n\nLoading the relevant data from the database file involves a combination of joins and subqueries. First, I have to find the exercise IDs corresponding to the exercises which have a “… (Warm Up)” version, and then I can retrieve the relevant entries from the training_log table. This gives the desired load_data() function.\n\n\nexercise_ids_subquery=\"\"\"\nSELECT _id\nFROM exercise\nWHERE name LIKE '% (Warm Up)'\n   OR name || ' (Warm Up)' IN (SELECT name FROM exercise)\n\"\"\"\n\nall_sets_query = f\"\"\"\nSELECT\n    T._id as \"set_id\",\n    T.date as \"date\",\n    E._id as \"exc_id\",\n    E.name as \"label\",\n    T.metric_weight as \"weight\",\n    T.reps as \"reps\"\nFROM\n    training_log T,\n    exercise E\nWHERE\n    T.exercise_id = E._id AND\n    T.exercise_id IN ({exercise_ids_subquery}) AND\n    E.name NOT LIKE 'Fake%'\n\"\"\"\n\ndef load_data(fitnotes_backup=FITNOTES_BACKUP):\n    all_sets = run_query(all_sets_query, db=fitnotes_backup)\n    return all_sets\n\nBefore I take a look at the data, I want to determine the date that I started introducing the warm-up categories. This involves another simple join query.\n\ndef get_split_date(db_file):\n    first_warm_up_query = \"\"\"\n    SELECT\n        min(T.date) as \"date\"\n    FROM \n        training_log T, exercise E\n    WHERE\n        T.exercise_id = E._id AND E.name LIKE \"% (Warm Up)\"\n    \"\"\"\n    result = run_query(first_warm_up_query, db_file)\n    return result['date'][0]\n\nprint(\"First appearance of warm-up label:\", get_split_date(FITNOTES_BACKUP))\n\nFirst appearance of warm-up label: 2020-02-07\n\n\nI will use this date to split the data in two parts: the properly labeled “good data”, and the partially incorrectly labeled “problem data”.\n\nSPLIT_DATE = get_split_date(FITNOTES_BACKUP)\n\ndef good_problem_split(data):\n    # extract good data\n    good_mask = data['date'] &gt;= SPLIT_DATE\n    good_data = data[good_mask]\n    # extract problematic data\n    problem_mask = data['date'] &lt; SPLIT_DATE\n    problem_data = data[problem_mask]\n    return good_data, problem_data"
  },
  {
    "objectID": "posts/fixing_my_fitnotes_db.html#step-2-exploring-and-preparing-the-data",
    "href": "posts/fixing_my_fitnotes_db.html#step-2-exploring-and-preparing-the-data",
    "title": "Fixing a Database Using Machine Learning",
    "section": "Step 2: Exploring and Preparing the Data",
    "text": "Step 2: Exploring and Preparing the Data\nFirst, it’s good to know how much data there is and how much of it is properly labeled.\n\n# load and split the data\nraw_data = load_data()\ngood_sets, problem_sets = good_problem_split(raw_data)\n# print the counts of elements\nprint(\"Numbers of tracked sets:\")\nprint(\"----------------------------\")\nprint(f\"{'Total sets:':17}{len(raw_data):4d}\")\nprint(f\"{'Good sets:':17}{len(good_sets):4d} ({100 * len(good_sets) / len(raw_data):0.0f} %)\")\nprint(f\"{'Problem sets:':17}{len(problem_sets):4d} ({100 * len(problem_sets) / len(raw_data):0.0f} %)\")\n\nNumbers of tracked sets:\n----------------------------\nTotal sets:      5091\nGood sets:       4057 (80 %)\nProblem sets:    1034 (20 %)\n\n\nNext up is a look at the good sets, if only to make sure that the split was done correctly. This is indeed so, since all exercises conistently start appearing with separate warm-up categories from SPLIT_DATE onnward:\n\ngood_sets.head(15)\n\n\n\n\n\n\n\n\nset_id\ndate\nexc_id\nlabel\nweight\nreps\n\n\n\n\n1034\n6832\n2020-02-07\n112\nFlat Barbell Bench Press (Warm Up)\n20.0\n6\n\n\n1035\n6836\n2020-02-07\n37\nFlat Barbell Bench Press\n50.0\n5\n\n\n1036\n6837\n2020-02-07\n37\nFlat Barbell Bench Press\n50.0\n5\n\n\n1037\n6838\n2020-02-07\n37\nFlat Barbell Bench Press\n50.0\n5\n\n\n1038\n6839\n2020-02-07\n37\nFlat Barbell Bench Press\n42.0\n9\n\n\n1039\n6841\n2020-02-07\n113\nDeadlift (Warm Up)\n60.0\n5\n\n\n1040\n6846\n2020-02-07\n48\nDeadlift\n106.0\n4\n\n\n1041\n6847\n2020-02-07\n48\nDeadlift\n106.0\n4\n\n\n1042\n6848\n2020-02-07\n48\nDeadlift\n104.0\n5\n\n\n1043\n6849\n2020-02-07\n48\nDeadlift\n90.0\n6\n\n\n1044\n6850\n2020-02-07\n112\nFlat Barbell Bench Press (Warm Up)\n40.0\n4\n\n\n1045\n6851\n2020-02-07\n112\nFlat Barbell Bench Press (Warm Up)\n50.0\n2\n\n\n1046\n6852\n2020-02-07\n113\nDeadlift (Warm Up)\n90.0\n2\n\n\n1047\n6853\n2020-02-07\n113\nDeadlift (Warm Up)\n100.0\n2\n\n\n1048\n6858\n2020-02-11\n111\nOverhead Press (Warm Up)\n20.0\n7\n\n\n\n\n\n\n\nUltimately, I’m interested in the information whether a given record represents a warm-up or a regular set. Whenever this information is known (i.e. for the good data), it is contained in the 'label' column (and implicitly in 'exc_id'), which also contains the information about the type of exercise (i.e. bench press, deadlift, overhead press, or squat). As is, the information is not presented in the most useful way. The same can be said about the columns 'set_id' and 'date'. In fact, the concrete values in these columns do not seem particularly relevant, but they do contain more obviously relevant information: the sets can be grouped by 'date' and the exercise type as extracted from 'label' and then counted in order of increasing 'set_id'. I will thus introduce the following extra columns to represent the relevant information more directly:\n\n'type': The exercise type (i.e. bench press, deadlift, overhead press, or squat)\n'BP', 'DL', 'OP', 'SQ': Binary dummy labels for the categorical values in 'type'\n'warm_up': The exercise category (1: warm-up, 0: regular, &lt;NA&gt;: unknown)\n'grp': A count of groups of sets grouped by workout date and exercise type.\n'set_count': The set number of each set within its group.\n\n\ndef process_labels_etc(data_in):\n    # copy data\n    data_out = data_in.copy()\n    # add column for exercise type\n    EXC_TYPES = {\"F\":\"Bench Press\", \"D\":\"Deadlift\", \"O\":\"Overhead Press\", \"L\":\"Squat\"}\n    data_out['type'] = data_out['label'].str[0].map(EXC_TYPES)\n    # add dummy columns for exercise types\n    data_out['BP'] = data_out['label'].str.contains('Bench').astype('Int8')\n    data_out['DL'] = data_out['label'].str.contains('Deadlift').astype('Int8')\n    data_out['OP'] = data_out['label'].str.contains('Overhead').astype('Int8')\n    data_out['SQ'] = data_out['label'].str.contains('Squat').astype('Int8')\n    # add dummy column for warm-up sets (1: warm-up, 0: regular, &lt;NA&gt;: unknown)\n    split_mask = data_out['date'] &gt;= SPLIT_DATE\n    warm_up_labels = data_out.loc[split_mask, 'label'].str.contains('Warm Up')\n    data_out.loc[split_mask, 'warm_up'] = warm_up_labels\n    data_out['warm_up'] = data_out['warm_up'].astype('Int8')\n    # group the sets by date and exercise type and count the gorups\n    data_out['grp'] = data_out.groupby(['date', 'type']).ngroup() + 1\n    # also count within each group\n    data_out['set_count'] = data_out.groupby(['date', 'type']).cumcount() + 1\n    return data_out\n\nThe main task is then to infer the values in warm_up from the other columns, excluding exc_id and label, because the latter are not reliable for the problem sets. In fact, the equivalent information in label and exc_id literally is the problem for the problem sets.\nSpeaking of problems, the numerical column 'weight' is also a little flawed, since the values for different exercise types lie in rather different ranges:\n\nraw_data = load_data()\nprepared_data = process_labels_etc(raw_data)\n\nprint(\"Average weight by exercise type:\")\nprint(prepared_data.groupby('type')['weight'].mean())\n\nAverage weight by exercise type:\ntype\nBench Press       45.126016\nDeadlift          90.507648\nOverhead Press    32.532346\nSquat             59.207232\nName: weight, dtype: float64\n\n\nThis can be remedied by suitably normalizing the weight. I find it most reasonable to normalize the weight on a per-group basis and I will do the same for the 'reps' column to put the parameter on a comparable scale. While doing so, I will also compute the set volume (i.e. weight times reps) and a per-group normalized version thereof.\n\ndef add_normalized_features(data_in):\n    # copy original dataframe to avoid unwanted changes\n    data_out = data_in.copy()\n    # add the set volume (weight * reps) as a column\n    data_out['set_vol'] = data_out['weight'] * data_out['reps']\n    # add per-group normalized weight, reps, and set volume columns\n    max_weight = data_out.groupby('grp')['weight'].transform('max')\n    max_reps = data_out.groupby('grp')['reps'].transform('max')\n    max_set_vol = data_out.groupby('grp')['set_vol'].transform('max')\n    data_out['norm_weight'] = data_out['weight'] / max_weight\n    data_out['norm_reps'] = data_out['reps'] / max_reps\n    data_out['norm_set_vol'] = data_out['set_vol'] / max_set_vol\n    return data_out\n\nTogether, the functions process_labels_etc and add_normalized_features make up the prepare_data function mentioned in the introduction.\n\ndef prepare_data(data_in):\n    data_out = process_labels_etc(data_in)\n    data_out = add_normalized_features(data_out)\n    return data_out\n\nThe following plots show the weight (blue) and reps (orange) for randomly chosen examples of set groups, both normalized on a per-group level. The main observation is that, while the per-exercise-routine within the problem data is not as consistent as within the good data, the differences are not too alarming. Chances are that a properly trained prediction model trained on the good data will also perform reasonably well on the problem data.\n\nimport random\n\n# load, prepare, and split all sets\nraw_data = load_data()\nprepared_data = prepare_data(raw_data)\ngood_data, problems = good_problem_split(prepared_data)\nsplit_data = {\n    \"Problem Sets\": problems,\n    \"Properly Labeled Sets\": good_data\n}\n\n# get samples\nEXC_TYPES = prepared_data['type'].unique()\nM = len(EXC_TYPES)\nN = 4\ngroup_samples = {}\nfor category in split_data:\n    data = split_data[category]\n    data_samples = []\n    for exc_type in EXC_TYPES:\n        exc_type_samples = []\n        df = data[data['type'] == exc_type]\n        sample_groups = df['grp'].sample(n=N, random_state=42)\n        sample_groups.sort_values(inplace=True)\n        for grp in sample_groups:\n            grp_df = df[df['grp']==grp]\n            exc_type_samples.append(grp_df)\n        data_samples.append(exc_type_samples)\n    group_samples[category] = data_samples\n# plot weight and reps for the samples\nW, H = 3, 2\nfor category in group_samples:\n    fig, axes = plt.subplots(N, M, figsize=(M*W, N*H))\n    fig.suptitle(f\"Some Groups of {category}\")\n    for m in range(M):\n        for n in range(N):\n            df = group_samples[category][m][n].copy()\n            df.set_index('set_count', inplace=True)\n            date = df['date'].iloc[0]\n            exc_type = df['type'].iloc[0]\n            df = df[['norm_weight', 'norm_reps']]\n            axes[n, m] = df.plot.bar(\n                title=f\"{exc_type}, {date}\",\n                ax=axes[n, m],  \n                legend=False\n            )\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLastly, here are the correlations of the numerical feature columns with the 'warm_up':\n\n# reload, prepare, and split the data\nraw_data = load_data()\nprepared_data = prepare_data(raw_data)\ngood_data, problems = good_problem_split(prepared_data)\n\n# extract the numerical column\nnumerical_data = good_data.select_dtypes(include=['number'])\n\n# Compute absolute corellations\ncorellations = numerical_data.corr().abs()\n# Restrict to `is_warm_up` colum and sort by descreasing corellation\ncorellations = corellations['warm_up'].sort_values(ascending=False)\n\n# Print the results\nprint(\"Correllation of numerical columns with 'warm_up':\")\nprint(corellations)\n\nCorrellation of numerical columns with 'warm_up':\nwarm_up         1.000000\nset_count       0.782057\nexc_id          0.736627\nnorm_set_vol    0.722205\nnorm_weight     0.610597\nset_vol         0.535162\nnorm_reps       0.339426\nreps            0.335302\nweight          0.281679\ngrp             0.022953\nBP              0.021510\nset_id          0.020281\nSQ              0.013093\nOP              0.008936\nDL              0.000292\nName: warm_up, dtype: float64\n\n\nAmong other things, this shows that the original columns are not very strongly correlated to warm_up. Some of the new columns, especially 'set_count' and 'norm_set_vol' look much more promising.\nIt remains to explain what the new column are:\n\n'set_vol': The set volume, that is, the set weight times the number of repetitions.\n'norm_weight': The normalized weight computed per group.\n'norm_vol': The normalized set volume computed per group.\n\nThe high correlation of the set number is not surprising, because the warm-up sets should appear in the beginning of the respective groups - at least if the sets were tracked in order (which is mostly true, but not always). The higher correlation of the set volume then its factors weight and reps also makes sense, given the knowledge how I’ve mostly structured my workouts. The better performance of the normalized columns can be explained by the different weight ranges mentioned above."
  },
  {
    "objectID": "posts/fixing_my_fitnotes_db.html#step-3-model-training-and-evaluation",
    "href": "posts/fixing_my_fitnotes_db.html#step-3-model-training-and-evaluation",
    "title": "Fixing a Database Using Machine Learning",
    "section": "Step 3: Model Training and Evaluation",
    "text": "Step 3: Model Training and Evaluation\nWith slightly over 5000 samples, the dataset is small enough to test a variety of models. To reiterate, the task is to predict the correct value of 'warm_up' from a suitable selection of other columns. This is set up as a binary classification problem, giving a variety of models to choose from.\nI will first try a few of the classics: logistic regression, decision tree, random forest, k-nearest neighbor, and linear SVM. Each of these classifiers will be trained using the same cross validation scheme and evaluated using same metrics.\nMostly out of curiosity, I will also train a simple neural network to see how it performs. However, since the simple models already perform extremely well, the deep learning approach will eventually be discarded.\nTo begin with, I’ll get the data ready. I’ll do a split into training, test, and validation data.\n\n# load and prepare data, split off good data\nraw_data = load_data()\nprepared_data = prepare_data(raw_data)\ngood_data, _ = good_problem_split(prepared_data)\n\n# select features for treaining\nIGNORE_COLS = ['date', 'label', 'type', 'exc_id', 'set_id', 'grp', 'warm_up']\nX_good = good_data.drop(IGNORE_COLS, axis=1)\ny_good = good_data['warm_up']\n\n# split into train/test/validation sets\nX_train_test, X_val, y_train_test, y_val = \\\n    train_test_split(X_good, y_good, test_size=0.1, stratify=y_good, random_state=42)\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X_train_test, y_train_test, test_size=0.2, stratify=y_train_test, random_state=42)\n\n\nClassical Binary Classifiers\nAs mentioned, I want to try out and compare several classical models. For that purpose, I will load them into a dictionary.\n\n# Define models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, solver=\"lbfgs\"),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n    \"SVM (Linear)\": LinearSVC(max_iter=5000, dual=False),\n    \"Naive Bayes\": GaussianNB(),\n}\n\nFor model selection I will first use a 5-fold cross-validation strategy on the training data.\n\ndef evaluate_with_cv(models:dict)-&gt;None:\n    # Cross-validation strategy\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    # Evaluate all models\n    results = {}\n    print(\"Cross-validation results:\".upper())\n    for name, model in models.items():\n        scores = cross_val_score(model, X_train_test, y_train_test, cv=cv, scoring=\"accuracy\")\n        results[name] = (np.mean(scores), np.std(scores))\n        print(f\"{name:20s} | Accuracy: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n\nevaluate_with_cv(models)\n\nCROSS-VALIDATION RESULTS:\nLogistic Regression  | Accuracy: 0.987 ± 0.004\nDecision Tree        | Accuracy: 0.985 ± 0.004\nRandom Forest        | Accuracy: 0.992 ± 0.005\nKNN                  | Accuracy: 0.972 ± 0.003\nSVM (Linear)         | Accuracy: 0.988 ± 0.006\nNaive Bayes          | Accuracy: 0.980 ± 0.004\n\n\nAll models perform rather well, with the random forest classifier in the lead.\nNext I will check the performance on the test data and plot the confusion matrix in each case.\n\ndef plot_confusion(models):\n    # create a 2-by-3 grid for the six models\n    fig, axes = plt.subplots(2,3,figsize=(9,6), constrained_layout=True)\n    axes = axes.flatten()\n    # pupolate the axes\n    ax_idx=0\n    for name, model in models.items():\n        # train the model on full training set\n        model.fit(X_train, y_train)\n        # evaluate on test set\n        y_pred = model.predict(X_test)\n        # compute accuracy score and confusion matrix\n        accuracy = model.score(X_test, y_test)\n        conf_mat = confusion_matrix(y_test, y_pred)\n        conf_mat_disp = ConfusionMatrixDisplay(conf_mat)\n        # add the results to current subplot\n        axes[ax_idx].set_title(f\"{name}\\n(Accuracy: {accuracy:.3f})\")\n        conf_mat_disp.plot(ax=axes[ax_idx])\n        ax_idx += 1\n    plt.show()\n\nplot_confusion(models)\n\n\n\n\n\n\n\n\nAgain, the random forest model performs best with an exceptionally low number of false negatives.\n\n# select the winning model\nbest_model = models[\"Random Forest\"]\n\ndef validate_trained_model(model):\n    # evaluate on validation set\n    y_pred = model.predict(X_val)\n    # compute accuracy score and confusion matrix\n    accuracy = model.score(X_val, y_val)\n    conf_mat = confusion_matrix(y_val, y_pred)\n    fig, ax = plt.subplots()\n    conf_mat_disp = ConfusionMatrixDisplay(conf_mat)\n    conf_mat_disp.plot(ax=ax)\n    # plt.show()\n    ax.set_title(f\"Results on Validation Set\\n{model}\\n(Accuracy: {accuracy:.3f})\")\n\nvalidate_trained_model(best_model)\n\n\n\n\n\n\n\n\n\ndef get_groups_with_errors(model):\n    # evaluate on all good data\n    y_pred = model.predict(X_good)\n    # add results as column in copy of good data\n    good_data_plus = good_data.copy()\n    good_data_plus['pred'] = y_pred.astype('int8')\n    # get exercise groups in good data\n    groups = good_data['grp'].unique()\n    groups_with_errors = []\n    for grp in groups:\n        view_cols = ['set_count', 'date', 'type', 'weight', 'reps', 'warm_up', 'pred']\n        df = good_data_plus[good_data_plus['grp'] == grp][view_cols]\n        all_pred_correct = (df['warm_up'] == df['pred']).all()\n        if not all_pred_correct:\n            groups_with_errors.append(df)\n    return groups_with_errors\n\ngroups_with_errors = get_groups_with_errors(best_model)\nprint(\"Number of groups with errors:\".upper(), len(groups_with_errors))\nprint(\"\\nGroups with errors:\\n\".upper())\nfor group in groups_with_errors:\n    print(group)\n\nNUMBER OF GROUPS WITH ERRORS: 9\n\nGROUPS WITH ERRORS:\n\n      set_count        date      type  weight  reps  warm_up  pred\n1169          1  2020-06-08  Deadlift    60.0     5        1     1\n1170          2  2020-06-08  Deadlift    70.0     5        1     0\n1171          3  2020-06-08  Deadlift    80.0     5        0     0\n1172          4  2020-06-08  Deadlift    90.0     5        0     0\n1173          5  2020-06-08  Deadlift    90.0     5        0     0\n      set_count        date            type  weight  reps  warm_up  pred\n1374          1  2020-08-04  Overhead Press    20.0     5        1     1\n1375          2  2020-08-04  Overhead Press    35.0     2        1     1\n1376          3  2020-08-04  Overhead Press    40.0     1        0     1\n1383          4  2020-08-04  Overhead Press    38.0     1        1     1\n1384          5  2020-08-04  Overhead Press    42.0     1        0     1\n1385          6  2020-08-04  Overhead Press    45.0     1        0     0\n1386          7  2020-08-04  Overhead Press    35.0     6        0     0\n      set_count        date   type  weight  reps  warm_up  pred\n1377          1  2020-08-04  Squat    20.0     5        1     1\n1378          2  2020-08-04  Squat    50.0     3        1     1\n1379          3  2020-08-04  Squat    70.0     2        0     1\n1380          4  2020-08-04  Squat    71.0     1        0     1\n1381          5  2020-08-04  Squat    72.0     1        0     0\n1382          6  2020-08-04  Squat    73.0     1        0     0\n1387          7  2020-08-04  Squat    70.0     1        1     1\n1388          8  2020-08-04  Squat    74.0     1        0     0\n1389          9  2020-08-04  Squat    75.0     1        0     0\n1390         10  2020-08-04  Squat    65.0     7        0     0\n      set_count        date         type  weight  reps  warm_up  pred\n1391          1  2020-08-06  Bench Press    20.0     5        1     1\n1392          2  2020-08-06  Bench Press    40.0     5        1     1\n1393          3  2020-08-06  Bench Press    50.0     1        0     1\n1394          4  2020-08-06  Bench Press    52.0     1        0     0\n1395          5  2020-08-06  Bench Press    55.0     1        0     0\n1396          6  2020-08-06  Bench Press    57.0     1        0     0\n1397          7  2020-08-06  Bench Press    58.0     1        0     0\n1401          8  2020-08-06  Bench Press    60.0     1        0     0\n1402          9  2020-08-06  Bench Press    62.0     1        0     0\n1403         10  2020-08-06  Bench Press    52.0     5        0     0\n      set_count        date      type  weight  reps  warm_up  pred\n1398          1  2020-08-06  Deadlift    60.0     5        1     1\n1399          2  2020-08-06  Deadlift    90.0     2        1     1\n1400          3  2020-08-06  Deadlift   100.0     1        0     1\n1404          4  2020-08-06  Deadlift   105.0     1        0     0\n1405          5  2020-08-06  Deadlift   108.0     1        0     0\n1406          6  2020-08-06  Deadlift   110.0     1        0     0\n1407          7  2020-08-06  Deadlift   115.0     1        0     0\n1408          8  2020-08-06  Deadlift   118.0     1        0     0\n      set_count        date   type  weight  reps  warm_up  pred\n1655          1  2020-10-30  Squat    30.0     5        1     1\n1656          2  2020-10-30  Squat    60.0     3        1     1\n1657          3  2020-10-30  Squat    71.0     5        0     0\n1658          4  2020-10-30  Squat    71.0     5        0     0\n1659          5  2020-10-30  Squat    71.0     6        0     0\n1660          6  2020-10-30  Squat    60.0    10        0     0\n1662          7  2020-10-30  Squat    68.0     2        1     0\n      set_count        date         type  weight  reps  warm_up  pred\n2176          1  2021-11-16  Bench Press    20.0     7        1     1\n2177          2  2021-11-16  Bench Press    40.0     3        1     1\n2178          3  2021-11-16  Bench Press    50.0     3        0     1\n2179          4  2021-11-16  Bench Press    52.0     3        0     0\n2180          5  2021-11-16  Bench Press    53.0     3        0     0\n2181          6  2021-11-16  Bench Press    54.0     3        0     0\n2182          7  2021-11-16  Bench Press    56.0     3        0     0\n2183          8  2021-11-16  Bench Press    58.0     1        0     0\n2184          9  2021-11-16  Bench Press    60.0     1        0     0\n2185         10  2021-11-16  Bench Press    62.0     1        0     0\n      set_count        date         type  weight  reps  warm_up  pred\n4707          1  2025-02-11  Bench Press    20.0     8        1     1\n4708          2  2025-02-11  Bench Press    40.0     3        1     1\n4709          3  2025-02-11  Bench Press    50.0     1        1     1\n4710          4  2025-02-11  Bench Press    53.0     5        0     0\n4711          5  2025-02-11  Bench Press    53.0     5        0     0\n4712          6  2025-02-11  Bench Press    53.0     5        0     0\n4713          7  2025-02-11  Bench Press    50.0     6        0     0\n4714          8  2025-02-11  Bench Press    50.0     2        0     1\n      set_count        date            type  weight  reps  warm_up  pred\n4784          1  2025-03-21  Overhead Press    20.0    10        1     1\n4785          2  2025-03-21  Overhead Press    30.0     8        1     1\n4786          3  2025-03-21  Overhead Press    35.0     5        1     1\n4787          4  2025-03-21  Overhead Press    38.0     2        0     0\n4788          5  2025-03-21  Overhead Press    42.0     1        0     1\n4789          6  2025-03-21  Overhead Press    45.0     1        0     0\n4790          7  2025-03-21  Overhead Press    47.0     1        0     0\n4791          8  2025-03-21  Overhead Press    30.0    10        0     0\n\n\nAltogether, this is rather satisfactory. The errors are limited to nine exercise groups, six of which were “max out sessions” where I went for new personal bests for single repetitions. These groups are certainly outliers.\n\n\nA Deep Learning Approach\nJust out of curiosity, I will try training a neural network. The first task is to convert the data to PyTorch’s very own torch.tensor data type.\n\n# Convert to PyTorch tensors\ndef convert_pandas_to_torch(panda):\n    by_way_of_numpy = panda.values.astype(np.float64)\n    tensor = torch.tensor(by_way_of_numpy, dtype=torch.double)\n    return tensor\nX_train = convert_pandas_to_torch(X_train)\ny_train = convert_pandas_to_torch(y_train).unsqueeze(1)\nX_test = convert_pandas_to_torch(X_test)\ny_test = convert_pandas_to_torch(y_test).unsqueeze(1)\nX_val = convert_pandas_to_torch(X_val)\ny_val = convert_pandas_to_torch(y_val).unsqueeze(1)\n\nAs for the neural network model, I will go for three linear layers with sigmoid activation functions. Since I don’t have a ton of training data, I will keep the number of parameters manageable.\n\n# define a simple neural network architecture\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 16),\n            nn.Sigmoid(),\n            nn.Linear(16, 8),\n            nn.Sigmoid(),\n            nn.Linear(8, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return self.net(x)\n\n# define model, loss function, and optimizer\nmodel_nn = SimpleNN(input_dim=X_train.shape[1]).double()\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model_nn.parameters(), lr=0.01)\n\nprint(\"Number of parameters:\", sum(p.numel() for p in model_nn.parameters()))\nprint(\"Neural Network Architecture:\")\nprint(model_nn)\n\nNumber of parameters: 337\nNeural Network Architecture:\nSimpleNN(\n  (net): Sequential(\n    (0): Linear(in_features=11, out_features=16, bias=True)\n    (1): Sigmoid()\n    (2): Linear(in_features=16, out_features=8, bias=True)\n    (3): Sigmoid()\n    (4): Linear(in_features=8, out_features=1, bias=True)\n    (5): Sigmoid()\n  )\n)\n\n\n\ndef train_and_evaluate_nn(model_nn, epochs=10001):\n    # train the neural network\n    for epoch in range(epochs):\n        model_nn.train()\n        optimizer.zero_grad()\n        outputs = model_nn(X_train)\n        loss = criterion(outputs, y_train)\n        loss.backward()\n        optimizer.step()\n        if epoch % 1000 == 0:\n            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n    # evaluate classification results\n    model_nn.eval()\n    with torch.no_grad():\n        preds = model_nn(X_test)\n        preds_binary = (preds &gt; 0.5).int()\n        accuracy = (preds_binary.squeeze() == y_test.squeeze().int()).float().mean().item()\n        print(f\"Test Accuracy: {accuracy:.2%}\")\n        conf_mat = confusion_matrix(preds_binary, y_test)\n        conf_mat_disp = ConfusionMatrixDisplay(conf_mat)\n        conf_mat_disp.plot()\n        plt.show()\n\n# run training and evaluation twice to compare results\nprint(\"---=== FIRST PASS ===---\")\ntrain_and_evaluate_nn(model_nn)\nprint(\"---=== SECOND PASS ===---\")\ntrain_and_evaluate_nn(model_nn)\n\n---=== FIRST PASS ===---\nEpoch 0, Loss: 0.7396\nEpoch 1000, Loss: 0.0169\nEpoch 2000, Loss: 0.0327\nEpoch 3000, Loss: 0.0265\nEpoch 4000, Loss: 0.0215\nEpoch 5000, Loss: 0.0290\nEpoch 6000, Loss: 0.0259\nEpoch 7000, Loss: 0.0237\nEpoch 8000, Loss: 0.0196\nEpoch 9000, Loss: 0.0176\nEpoch 10000, Loss: 0.0158\nTest Accuracy: 98.63%\n\n\n\n\n\n\n\n\n\n---=== SECOND PASS ===---\nEpoch 0, Loss: 0.0159\nEpoch 1000, Loss: 0.0146\nEpoch 2000, Loss: 0.0121\nEpoch 3000, Loss: 0.2707\nEpoch 4000, Loss: 0.0256\nEpoch 5000, Loss: 0.0239\nEpoch 6000, Loss: 0.0213\nEpoch 7000, Loss: 0.6787\nEpoch 8000, Loss: 0.6787\nEpoch 9000, Loss: 0.6787\nEpoch 10000, Loss: 0.6787\nTest Accuracy: 58.55%\n\n\n\n\n\n\n\n\n\nThe first result is actually quite promising. However, the second attempt clearly shows that things can go wrong when training neural networks. I could go on and investigate what went wrong (most likely a zero-gradient issue), but given the good performance of the much simpler classical models, I will not pursue the matter any further.\n\n\nThe Final Implementation\nBased on the experiments in the previous sections, I will use a random forest classifier to correct the wrond exercise IDs in the “problem data”.\n\ndef predict_warm_ups(prepared_data):\n    # split data\n    good_data, _ = good_problem_split(prepared_data)\n    # extract features X and target y\n    IGNORE_COLS = ['date', 'label', 'type', 'exc_id', 'set_id', 'grp', 'warm_up']\n    X_good = good_data.drop(IGNORE_COLS, axis=1)\n    y_good = good_data['warm_up']\n    # train random forest model\n    random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n    warm_up_predictor = random_forest.fit(X_good, y_good)\n    # make predictions and add in column 'pred'\n    X_all = prepared_data.drop(IGNORE_COLS, axis=1)\n    y_all_pred = warm_up_predictor.predict(X_all)\n    # add prediction colun to copy of data and return\n    data_pred = prepared_data.copy()\n    data_pred['pred'] = pd.Series(y_all_pred).astype('Int8')\n    return data_pred"
  },
  {
    "objectID": "posts/fixing_my_fitnotes_db.html#step-4-fixing-the-database",
    "href": "posts/fixing_my_fitnotes_db.html#step-4-fixing-the-database",
    "title": "Fixing a Database Using Machine Learning",
    "section": "Step 4: Fixing the Database",
    "text": "Step 4: Fixing the Database\nIt remains to update the database file using the predicted exercise IDs for the “problem sets” recorded before the SPLIT_DATE. This process is straight forward.\nFirst, two helper functions are needed: one to obtain IDs of the “bad sets”, that is, those that need to be changed, and another one to determine the new exercise ID from the old one.\n\ndef get_bad_set_ids(data_pred):\n    _, problems = good_problem_split(data_pred)\n    bad_set_ids = problems.query('pred == 1')['set_id']\n    return bad_set_ids\n\n\nnew_exc_id_query = \"\"\"\nSELECT \n    W._id as \"new_id\"\nFROM\n    exercise E, \n    exercise W, \n    training_log T\nWHERE \n    W.name = E.name || ' (Warm Up)' AND\n    E._id = T.exercise_id AND \n    T._id = {:s}\n\"\"\"\n\ndef get_warm_up_id(set_id:int)-&gt;int:\n    id_df = run_query(new_exc_id_query.format(f\"{set_id}\"))\n    warm_up_exc_id = id_df['new_id'][0]\n    return warm_up_exc_id\n\nWith the bad set IDs and their corresponding new exercise IDs available, the database can be updated. To avoid data loss, I will implement an option that by default creates a new file with the updated IDs, leaving the original file untouched.\n\ndef update_db_file(db_file, bad_ids, inplace=False):\n    # create copy for update unless 'inplace' options\n    if not inplace:\n        original_db_file = db_file\n        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n        db_file = original_db_file.replace(\n            '.fitnotes', \n            f'__exc_ids_fixed_{timestamp}.fitnotes'\n        )\n        shutil.copyfile(original_db_file, db_file)\n    # update the db entries\n    update_query_template = \"\"\"\n    UPDATE training_log\n    SET exercise_id = {exc_id:s}\n    WHERE _id = {set_id};\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        cur = conn.cursor()\n        for set_id in bad_ids:\n            new_exc_id = get_warm_up_id(set_id)\n            update_query = update_query_template.format(\n                set_id=str(set_id),\n                exc_id=str(new_exc_id),\n            )\n            cur.execute(update_query)\n        conn.commit()\n\nNow everything needed for the main function fix_db_file is ready for assembly.\n\ndef fix_db_file(db_file=FITNOTES_BACKUP, inplace=False):\n    # data gathering\n    data = load_data(db_file)\n    # prepare data\n    prepared_data = prepare_data(data)\n    # make prediction\n    predicted_data = predict_warm_ups(prepared_data)\n    bad_set_ids = get_bad_set_ids(predicted_data)\n    # update database file\n    update_db_file(db_file, bad_set_ids, inplace=inplace)\n    return predicted_data\n\nfinal_data = fix_db_file()\n\nFinally, here’s how the model performed on the last 25 “problem sets”.\n\n_, problem_data = good_problem_split(final_data)\nview_cols = ['date','type','set_count','weight','reps','pred']\nproblem_data[view_cols].tail(25)\n\n\n\n\n\n\n\n\ndate\ntype\nset_count\nweight\nreps\npred\n\n\n\n\n1009\n2020-01-31\nDeadlift\n7\n105.0\n5\n0\n\n\n1010\n2020-01-31\nDeadlift\n8\n90.0\n10\n0\n\n\n1011\n2020-01-31\nBench Press\n4\n52.0\n3\n0\n\n\n1012\n2020-02-02\nBench Press\n1\n20.0\n5\n1\n\n\n1013\n2020-02-02\nBench Press\n2\n40.0\n3\n1\n\n\n1014\n2020-02-02\nBench Press\n3\n50.0\n2\n1\n\n\n1015\n2020-02-02\nBench Press\n4\n50.0\n6\n0\n\n\n1016\n2020-02-02\nBench Press\n5\n50.0\n3\n0\n\n\n1017\n2020-02-02\nBench Press\n6\n47.0\n5\n0\n\n\n1018\n2020-02-02\nBench Press\n7\n47.0\n5\n0\n\n\n1019\n2020-02-02\nBench Press\n8\n42.0\n8\n0\n\n\n1020\n2020-02-04\nOverhead Press\n1\n20.0\n5\n1\n\n\n1021\n2020-02-04\nOverhead Press\n2\n30.0\n3\n1\n\n\n1022\n2020-02-04\nOverhead Press\n3\n35.0\n2\n1\n\n\n1023\n2020-02-04\nOverhead Press\n4\n38.0\n5\n0\n\n\n1024\n2020-02-04\nOverhead Press\n5\n38.0\n5\n0\n\n\n1025\n2020-02-04\nOverhead Press\n6\n38.0\n5\n0\n\n\n1026\n2020-02-04\nOverhead Press\n7\n28.0\n8\n0\n\n\n1027\n2020-02-04\nSquat\n1\n28.0\n6\n1\n\n\n1028\n2020-02-04\nSquat\n2\n50.0\n2\n1\n\n\n1029\n2020-02-04\nSquat\n3\n65.0\n1\n1\n\n\n1030\n2020-02-04\nSquat\n4\n65.0\n5\n0\n\n\n1031\n2020-02-04\nSquat\n5\n65.0\n5\n0\n\n\n1032\n2020-02-04\nSquat\n6\n65.0\n6\n0\n\n\n1033\n2020-02-04\nSquat\n7\n55.0\n12\n0\n\n\n\n\n\n\n\nAll in all, rather satisfactory."
  },
  {
    "objectID": "posts/talking_to_fritz.html",
    "href": "posts/talking_to_fritz.html",
    "title": "Talking to AMV FRITZ!Box Routers in Python",
    "section": "",
    "text": "Ever wondered how you can get Python to communicate with your AVM FRITZ!Box router? So have I. In this post I will discuss example implementations using both the TR-064 and the AHA-HTTP interfaces.\nThe code is extracted from my ongoing project sb4dfritz which is available on GitHub."
  },
  {
    "objectID": "posts/talking_to_fritz.html#introduction",
    "href": "posts/talking_to_fritz.html#introduction",
    "title": "Talking to AMV FRITZ!Box Routers in Python",
    "section": "Introduction",
    "text": "Introduction\nI have a small home automation system controlled by my AVM FRITZ!Box router. Since I had a few specific use cases that were not easily handled with the available mobile and web apps, I started working on custom Python tools to make my life easier. I might write more about the project in other posts. For now, I just want to focus on the basic communication between FRITZ!Box routers and Python.\nAVM offers to two different APIs for developers to interact with home automation devices:\n\nThe TR-064 interface is based on SOAP and the synonymous TR-064 protocol.\nThe AHA-HTTP interface where AHA stands for “AVM Home Automation”.\n\nBoth interfaces have methods to read out basic device statistics such as current power consumption for smart plugs or temperature for radiator controls. I will lay down the implementation in Python below."
  },
  {
    "objectID": "posts/talking_to_fritz.html#the-tr-064-interface",
    "href": "posts/talking_to_fritz.html#the-tr-064-interface",
    "title": "Talking to AMV FRITZ!Box Routers in Python",
    "section": "The TR-064 Interface",
    "text": "The TR-064 Interface\nUsing the TR-064 interface involves parsing an XML message in SOAP message format which is then sent via an HTTP POST request. The available actions relating to home automation are described here. However, I struggled to get anywhere with the official documentation. The code below is based on a working example which I found in a Google search.\nThe following data is needed for a request:\n\nValid login information (user name and password).\nThe IP address of the FRTITZ!Box router\nThe name of the SOAP action as listed in the documentation (below: GetSpecificDeviceInfos).\nAdditional parameters needed for the specific action also listed in the documentation (below: the device identifier number known as AIN).\n\nHere’s an implementation of the SOAP action GetSpecificDeviceInfos using the requests module:\n\nimport requests, warnings\nfrom requests.auth import HTTPDigestAuth\n\ndef get_specific_device_info(user:str, pwd:str, ip:str, device_ain:str)-&gt;requests.Response:\n    \"\"\"GetSpecificDeviceInfos action for TR-064 interfaces.\"\"\"\n    # constants needed for the request\n    UPNP_URL = \"https://\" + ip + \":49443/upnp/control/x_homeauto\"\n    TR064_SERVICE = \"urn:dslforum-org:service:X_AVM-DE_Homeauto:1\"\n    SOAP_ACTION = \"GetSpecificDeviceInfos\"\n    # header for POST request\n    request_headers = {\n        'Content-Type': 'text/xml; charset=\"utf-8\"', \n        'SoapAction': TR064_SERVICE + \"#\" + SOAP_ACTION\n    }\n    # data for POST request\n    request_data = f\"\"\"\n        &lt;?xml version=\\\"1.0\\\"?&gt; \n        &lt;s:Envelope \n         xmlns:s=\\\"http://schemas.xmlsoap.org/soap/envelope/\\\" \n         s:encodingStyle=\\\"http://schemas.xmlsoap.org/soap/encoding/\\\"&gt; \n            &lt;s:Body&gt; \n                &lt;u:{SOAP_ACTION} xmlns:u=\\\"{TR064_SERVICE}\\\"&gt; \n                    &lt;NewAIN&gt;{device_ain}&lt;/NewAIN&gt; \n                &lt;/u:{SOAP_ACTION}&gt; \n            &lt;/s:Body&gt; \n        &lt;/s:Envelope&gt;\n        \"\"\"\n    # temporary ignore warnings (caused by self-signed certificate of FRITZ!Box)\n    warnings.simplefilter('ignore')\n    # send POST request\n    request_result = requests.post(\n        url=UPNP_URL, \n        auth=HTTPDigestAuth(user, pwd), \n        headers=request_headers, \n        data=request_data, \n        verify=False\n    )\n    # allow warning again\n    warnings.resetwarnings()\n    return request_result\n\nA call to get_specific_device_info(...) returns a requests.Response object. The .text attribute contains the requested data, again in XML format. Here’s an example for illustration:\nresults = get_specific_device_info(\"user\", \"pwd\", \"ip\", \"12345 6789012\")\n\nprint(results.text)\n&lt;?xml version=\"1.0\"?&gt;\n&lt;s:Envelope xmlns:s=\"http://schemas.xmlsoap.org/soap/envelope/\" s:encodingStyle=\"http://schemas.xmlsoap.org/soap/encoding/\"&gt;\n&lt;s:Body&gt;\n&lt;u:GetSpecificDeviceInfosResponse xmlns:u=\"urn:dslforum-org:service:X_AVM-DE_Homeauto:1\"&gt;\n&lt;NewDeviceId&gt;18&lt;/NewDeviceId&gt;\n&lt;NewFunctionBitMask&gt;35712&lt;/NewFunctionBitMask&gt;\n&lt;NewFirmwareVersion&gt;04.27&lt;/NewFirmwareVersion&gt;\n&lt;NewManufacturer&gt;AVM&lt;/NewManufacturer&gt;\n&lt;NewProductName&gt;FRITZ!DECT 200&lt;/NewProductName&gt;\n&lt;NewDeviceName&gt;TV etc&lt;/NewDeviceName&gt;\n&lt;NewPresent&gt;CONNECTED&lt;/NewPresent&gt;\n&lt;NewMultimeterIsEnabled&gt;ENABLED&lt;/NewMultimeterIsEnabled&gt;\n&lt;NewMultimeterIsValid&gt;VALID&lt;/NewMultimeterIsValid&gt;\n&lt;NewMultimeterPower&gt;1130&lt;/NewMultimeterPower&gt;\n&lt;NewMultimeterEnergy&gt;1023099&lt;/NewMultimeterEnergy&gt;\n&lt;NewTemperatureIsEnabled&gt;ENABLED&lt;/NewTemperatureIsEnabled&gt;\n&lt;NewTemperatureIsValid&gt;VALID&lt;/NewTemperatureIsValid&gt;\n&lt;NewTemperatureCelsius&gt;235&lt;/NewTemperatureCelsius&gt;\n&lt;NewTemperatureOffset&gt;0&lt;/NewTemperatureOffset&gt;\n&lt;NewSwitchIsEnabled&gt;ENABLED&lt;/NewSwitchIsEnabled&gt;\n&lt;NewSwitchIsValid&gt;VALID&lt;/NewSwitchIsValid&gt;\n&lt;NewSwitchState&gt;ON&lt;/NewSwitchState&gt;\n&lt;NewSwitchMode&gt;MANUAL&lt;/NewSwitchMode&gt;\n&lt;NewSwitchLock&gt;1&lt;/NewSwitchLock&gt;\n&lt;NewHkrIsEnabled&gt;DISABLED&lt;/NewHkrIsEnabled&gt;\n&lt;NewHkrIsValid&gt;INVALID&lt;/NewHkrIsValid&gt;\n&lt;NewHkrIsTemperature&gt;0&lt;/NewHkrIsTemperature&gt;\n&lt;NewHkrSetVentilStatus&gt;CLOSED&lt;/NewHkrSetVentilStatus&gt;\n&lt;NewHkrSetTemperature&gt;0&lt;/NewHkrSetTemperature&gt;\n&lt;NewHkrReduceVentilStatus&gt;CLOSED&lt;/NewHkrReduceVentilStatus&gt;\n&lt;NewHkrReduceTemperature&gt;0&lt;/NewHkrReduceTemperature&gt;\n&lt;NewHkrComfortVentilStatus&gt;CLOSED&lt;/NewHkrComfortVentilStatus&gt;\n&lt;NewHkrComfortTemperature&gt;0&lt;/NewHkrComfortTemperature&gt;\n&lt;/u:GetSpecificDeviceInfosResponse&gt;\n&lt;/s:Body&gt;\n&lt;/s:Envelope&gt;"
  },
  {
    "objectID": "posts/talking_to_fritz.html#the-aha-http-interface",
    "href": "posts/talking_to_fritz.html#the-aha-http-interface",
    "title": "Talking to AMV FRITZ!Box Routers in Python",
    "section": "The AHA-HTTP Interface",
    "text": "The AHA-HTTP Interface\nUsing AHA-HTTP interface is syntactically much simpler. It involves parsing a URL with parameters which is then sent as an HTTP GET request. The caveat is that one of the parameters is a valid session ID (SID) which has to be obtained by a separate login procedure. The implementation of the latter is somewhat involved. I’ll get to it later.\nAssuming that a valid SID is already available, the implementation is straight forward. I’ll use the command 'getbasicdevicestats' which requires the additional parameters 'sid' and 'ain'.\n\nimport requests\n\nURL_BASE = 'http://fritz.box/webservices/homeautoswitch.lua'\n\ndef getbasicdevicestats(ain:str, sid:str)-&gt;dict:\n    \"\"\"Get basic statistic (temperature, power, voltage, energy) of device.\"\"\"\n    # assemble parameter dictionary according to AHA-HTTP documentation\n    params = {\n        'switchcmd': 'getbasicdevicestats', \n        'ain':ain,\n        'sid':sid, \n    }\n    # send GET request (verify=False for self-signed certificate)\n    response = requests.get(url=URL_BASE, params=params, verify=False)\n    return response\n\nAs with the TR-064 implementation, the response comes in the form of a requests.Response object with XML formatted data in the .text attribute. Here’s an example. The get_sid(...) function is provided in the next section.\n# get a session ID\nsid = get_sid(\"user\", \"pwd\")\n# call the implementation of 'getbasicdevicestats'\nresults = getbasicdevicestats(\"12345 6789012\", sid)\nprint(results.text)\n&lt;devicestats&gt;\n    &lt;temperature&gt;\n        &lt;stats count=\"96\" grid=\"900\" datatime=\"1757511566\"&gt;\n            235,235,235,230,230,225,235,235,235,235,235,235,235,235,235,235,\n            235,235,235,235,235,235,235,230,230,230,230,225,220,225,235,235,\n            235,235,235,235,235,235,235,235,235,235,235,235,235,235,235,235,\n            235,235,235,235,235,235,235,235,235,235,235,235,235,235,235,235,\n            235,235,235,235,235,235,235,235,235,235,235,235,235,235,235,235,\n            235,235,235,235,235,235,235,235,235,235,235,235,235,235,235,235\n        &lt;/stats&gt;\n    &lt;/temperature&gt;\n\n    &lt;voltage&gt;\n        &lt;stats count=\"360\" grid=\"10\" datatime=\"1757511566\"&gt;\n            231075,231075,231075,231075,231075,231075,231075,231075,231075,231075,\n            231075,231075,231136,231136,231136,231136,231136,231136,231136,231136,\n            231136,231136,231136,231136,229981,229981,229981,229981,229981,229981,\n            ...\n            230001,230001,230001,230001,230001,230001,230001,230001,230001,230001,\n            230001,230001\n        &lt;/stats&gt;\n    &lt;/voltage&gt;\n\n    &lt;power&gt;\n        &lt;stats count=\"360\" grid=\"10\" datatime=\"1757511566\"&gt;\n            1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,\n            1130,1130,1130,1130,1130,1130,1130,1130,1130,1130,1130,1130,\n            ...\n            1137,1137,1137,1137,1137,1137,1137,1137,1137,1137,1137,1137\n        &lt;/stats&gt;\n    &lt;/power&gt;\n\n    &lt;energy&gt;\n        &lt;stats count=\"12\" grid=\"2678400\" datatime=\"1757462405\"&gt;\n            3876,11670,10987,12726,12952,11345,10704,13675,12599,15803,8513,7917\n        &lt;/stats&gt;\n        &lt;stats count=\"31\" grid=\"86400\" datatime=\"1757462404\"&gt;\n            148,259,493,99,98,137,384,481,1117,660,233,188,591,558,350,444,\n            521,671,756,264,204,392,312,326,0,0,58,323,375,400,489\n        &lt;/stats&gt;\n    &lt;/energy&gt;\n&lt;/devicestats&gt;\n\nObtaining a session ID\nLastly, it remains to discuss login procedure to obtain a valid session ID. An official Python implementation is provided in this document.\n\n\"\"\"\nFRITZ!OS WebGUI Login (modified)\nGet a sid (session ID) via PBKDF2 based challenge response algorithm.\nFallback to MD5 if FRITZ!OS has no PBKDF2 support.\nAVM 2020-09-25 (code base)\nStefan Behrens 2025-08-19 (modifications)\n\"\"\"\n\nimport sys\nimport hashlib\nimport time\nimport urllib.request\nimport urllib.parse\nimport xml.etree.ElementTree as ET\n\nLOGIN_SID_ROUTE = \"/login_sid.lua?version=2\"\n\nclass LoginState:\n    def __init__(self, challenge: str, blocktime: int):\n        self.challenge = challenge\n        self.blocktime = blocktime\n        self.is_pbkdf2 = challenge.startswith(\"2$\")\n\n\ndef check_sid_validity(sid:str|int, address:str=\"fritz.box\")-&gt;bool:\n    \"\"\"Check if the given SID is valid.\n    \n    Args\n    - sid : 16 digit integer (possibly formatted as string)\n    - address : FRITZ!Box IP or address (optional, default: fritz.box)\n    \n    Returns\n    - sid_is_valid : Boolean, True if SID is valid, False else\n    \"\"\"\n    url = f\"http://{address}/login_sid.lua?version=2&sid={sid}\"\n    resp = urllib.request.urlopen(url)\n    root = ET.fromstring(resp.read())\n    sid_value = root.find(\"SID\").text\n    sid_is_valid = (sid_value != \"0000000000000000\")\n    return sid_is_valid\n\n\ndef get_sid(username: str, password: str, address:str=\"fritz.box\") -&gt; str:\n    \"\"\" Get a sid by solving the PBKDF2 (or MD5) challenge-response\n    process. \"\"\"\n    box_url = \"http://\" + address\n    try:\n        state = get_login_state(box_url)\n    except Exception as ex:\n        raise Exception(\"failed to get challenge\") from ex\n    if state.is_pbkdf2:\n        # print(\"PBKDF2 supported\")\n        challenge_response = calculate_pbkdf2_response(state.challenge, password)\n    else:\n        # print(\"Falling back to MD5\")\n        challenge_response = calculate_md5_response(state.challenge, password)\n    if state.blocktime &gt; 0:\n        # print(f\"Waiting for {state.blocktime} seconds...\")\n        time.sleep(state.blocktime)\n    try:\n        sid = send_response(box_url, username, challenge_response)\n    except Exception as ex:\n        raise Exception(\"failed to login\") from ex\n    if sid == \"0000000000000000\":\n        raise Exception(\"wrong username or password\")\n    return sid\n\n\ndef get_login_state(box_url: str) -&gt; LoginState:\n    \"\"\" Get login state from FRITZ!Box using login_sid.lua?version=2 \"\"\"\n    url = box_url + LOGIN_SID_ROUTE\n    http_response = urllib.request.urlopen(url)\n    xml = ET.fromstring(http_response.read())\n    # print(f\"xml: {xml}\")\n    challenge = xml.find(\"Challenge\").text\n    blocktime = int(xml.find(\"BlockTime\").text)\n    return LoginState(challenge, blocktime)\n\n\ndef calculate_pbkdf2_response(challenge: str, password: str) -&gt; str:\n    \"\"\" Calculate the response for a given challenge via PBKDF2 \"\"\"\n    challenge_parts = challenge.split(\"$\")\n    # Extract all necessary values encoded into the challenge\n    iter1 = int(challenge_parts[1])\n    salt1 = bytes.fromhex(challenge_parts[2])\n    iter2 = int(challenge_parts[3])\n    salt2 = bytes.fromhex(challenge_parts[4])\n    # Hash twice, once with static salt...\n    hash1 = hashlib.pbkdf2_hmac(\"sha256\", password.encode(), salt1, iter1)\n    # Once with dynamic salt.\n    hash2 = hashlib.pbkdf2_hmac(\"sha256\", hash1, salt2, iter2)\n    return f\"{challenge_parts[4]}${hash2.hex()}\"\n\n\ndef calculate_md5_response(challenge: str, password: str) -&gt; str:\n    \"\"\" Calculate the response for a challenge using legacy MD5 \"\"\"\n    response = challenge + \"-\" + password\n    # the legacy response needs utf_16_le encoding\n    response = response.encode(\"utf_16_le\")\n    md5_sum = hashlib.md5()\n    md5_sum.update(response)\n    response = challenge + \"-\" + md5_sum.hexdigest()\n    return response\n\n\ndef send_response(box_url: str, username: str, challenge_response: str)-&gt;str:\n    \"\"\" Send the response and return the parsed sid. raises an Exception on\n    error \"\"\"\n    # Build response params\n    post_data_dict = {\"username\": username, \"response\": challenge_response}\n    post_data = urllib.parse.urlencode(post_data_dict).encode()\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    url = box_url + LOGIN_SID_ROUTE\n    # Send response\n    http_request = urllib.request.Request(url, post_data, headers)\n    http_response = urllib.request.urlopen(http_request)\n    # Parse SID from resulting XML.\n    xml = ET.fromstring(http_response.read())\n    return xml.find(\"SID\").text"
  }
]